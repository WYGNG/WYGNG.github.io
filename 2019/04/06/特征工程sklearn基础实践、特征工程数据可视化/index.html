<!DOCTYPE HTML>
<html lang="zh_CN">


<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="keywords" content="特征工程sklearn基础实践、特征工程数据可视化, 鱼缸屋">
    <meta name="description" content="数据和特征决定了机器学习的上限,而模型和算法只是逼近这个上限而已。
特征工程sklearn实践这里主要以iris数据集为例，展示了数据预处理方法、三大类特征选择方法、降维方法。数据预处理方法:

z_score标准化；
min_max标准化">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>特征工程sklearn基础实践、特征工程数据可视化 | 鱼缸屋</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/css/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    <style type="text/css">
        
    </style>

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
</head>


<body>

<header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="container">
            <div class="nav-wrapper">
                <div class="brand-logo">
                    <a href="/" class="waves-effect waves-light">
                        
                        <img src="/medias/logo.png" class="logo-img hide-on-small-only">
                        
                        <span class="logo-span">鱼缸屋</span>
                    </a>
                </div>
                

<a href="#" data-activates="mobile-nav" class="button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-home"></i>
            
            <span>首页</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>标签</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-bookmark"></i>
            
            <span>分类</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-archive"></i>
            
            <span>归档</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <i class="fa fa-user-circle-o"></i>
            
            <span>关于</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/friends" class="waves-effect waves-light">
            
            <i class="fa fa-address-book"></i>
            
            <span>友情链接</span>
        </a>
    </li>
    
    <li>
        <a id="toggleSearch" class="waves-effect waves-light">
            <i id="searchIcon" class="mdi-action-search" title="Search"></i>
        </a>
    </li>

</ul>

<div class="side-nav" id="mobile-nav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">鱼缸屋</div>
        <div class="logo-desc">
            
            海纳百川，有容乃大，壁立千仞，无欲则刚
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-home"></i>
                
                首页
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                标签
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-bookmark"></i>
                
                分类
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-archive"></i>
                
                归档
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-user-circle-o"></i>
                
                关于
            </a>
        </li>
        
        <li>
            <a href="/friends" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-address-book"></i>
                
                友情链接
            </a>
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/WYGNG" class="waves-effect waves-light" target="_blank">
                <i class="fa fa-github-square fa-fw"></i>来github一起玩吧！
            </a>
        </li>
        
    </ul>

    <div class="social-link">
    <a href="https://github.com/wygny" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:ygwu@mail.ustc.edu.cn" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=321699849" class="tooltipped" data-tooltip="QQ联系我: 321699849" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>



    <a href="https://www.baidu.com" class="tooltipped" target="_blank" data-tooltip="访问百度" data-position="top" data-delay="50">
        <i class="fa fa-paw"></i>
</a>



    <a href="https://www.google.com" class="tooltipped" target="_blank" data-tooltip="访问谷歌" data-position="top" data-delay="50">
        <i class="fa fa-google"></i>
</a>



    <a href="https://www.bilibili.com" class="tooltipped" target="_blank" data-tooltip="访问哔哩哔哩" data-position="top" data-delay="50">
        <i class="fa fa-bullseye"></i>
</a>



    <a href="https://blog.csdn.net/zgcr654321" class="tooltipped" target="_blank" data-tooltip="访问我的CSDN博客" data-position="top" data-delay="50">
        <i class="fa fa-copyright"></i>
</a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>




</div>
</div>

            </div>
        </div>

        
        <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/WYGNG" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="来github一起玩吧！" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>
</header>





<div class="bg-cover post-cover" style="background-image: url('/medias/featureimages/21.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        特征工程sklearn基础实践、特征工程数据可视化
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }
</style>
<div class="row">
    <div class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/特征工程/" target="_blank">
                                <span class="chip bg-color">特征工程</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/特征工程/" class="post-category" target="_blank">
                                特征工程
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2019-04-06
                </div>

                
                    
                    <div class="info-break-policy">
                        <i class="fa fa-file-word-o fa-fw"></i>Word Count:&nbsp;&nbsp;
                        4.8k
                    </div>
                    

                    
                    <div class="info-break-policy">
                        <i class="fa fa-clock-o fa-fw"></i>Read Times:&nbsp;&nbsp;
                        19 Min
                    </div>
                    
                
				
				
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="fa fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <p><strong>数据和特征决定了机器学习的上限,而模型和算法只是逼近这个上限而已。</strong></p>
<h1 id="特征工程sklearn实践"><a href="#特征工程sklearn实践" class="headerlink" title="特征工程sklearn实践"></a>特征工程sklearn实践</h1><p>这里主要以iris数据集为例，展示了数据预处理方法、三大类特征选择方法、降维方法。<br><strong>数据预处理方法:</strong></p>
<ul>
<li>z_score标准化；</li>
<li>min_max标准化；</li>
<li>数据单位向量化；</li>
<li>数据二值化；</li>
<li>数据转为one_hot编码形式；</li>
<li>缺失值填充；</li>
<li>特征多项式变换；</li>
<li>特征使用自定义函数变换。</li>
</ul>
<p><strong>三大类特征选择方法:</strong></p>
<ul>
<li>(Filter)过滤法:特征打分，按阈值保留特征。包括方差选择特征法、卡方检验法、皮尔森相关系数法；</li>
<li>(Wrapper)包装法:根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征。包括递归特征消除法等。</li>
<li>(Embedded)嵌入法:使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。包括基于惩罚项的特征选择法和基于树模型的特征选择法。</li>
</ul>
<p><strong>降维方法:</strong></p>
<ul>
<li>PCA；</li>
<li>LDA。</li>
</ul>
<p><strong>代码实现如下:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, MinMaxScaler, Normalizer, Binarizer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder, PolynomialFeatures, FunctionTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold, SelectKBest, chi2, RFE, SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis <span class="keyword">as</span> LDA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以iris数据集为例</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">x, y = iris.data, iris.target</span><br><span class="line"><span class="comment"># 训练集与测试集7:3划分</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">0</span>)</span><br><span class="line">print(x_train[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据z_score标准化,x减均值再除以标准差</span></span><br><span class="line">ss = StandardScaler()</span><br><span class="line">x_z_score = ss.fit_transform(x_train)</span><br><span class="line"><span class="comment"># print(x_z_score[0])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据min_max标准化,(x-xmin)/(xmax-xmin)</span></span><br><span class="line">mm = MinMaxScaler()</span><br><span class="line">x_min_max = mm.fit_transform(x_train)</span><br><span class="line"><span class="comment"># print(x_min_max[0])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据单位化,norm可以为l1、l2或max，默认为l2</span></span><br><span class="line"><span class="comment"># 为l1时，样本各个特征值除以各个特征值的绝对值之和;为l2时，样本各个特征值除以各个特征值的平方之和;为max时，样本各个特征值除以样本中特征值最大的值</span></span><br><span class="line">nm = Normalizer(norm=<span class="string">"l2"</span>)</span><br><span class="line">x_normalize = nm.fit_transform(x_train)</span><br><span class="line"><span class="comment"># print(x_normalize[0])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据二值化，设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0</span></span><br><span class="line">x_binary = Binarizer(<span class="number">3</span>, ).fit_transform(x_train)</span><br><span class="line"><span class="comment"># print(x_binary[0:2])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将特征转为one_hot编码(也可用pandas.get_dummies函数)</span></span><br><span class="line">ohe = OneHotEncoder(categories=<span class="string">'auto'</span>, sparse=<span class="literal">False</span>)</span><br><span class="line">x_one_hot = ohe.fit_transform([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="comment"># print(x_one_hot[0])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 填充缺失值(也可用pandas.fillna函数),用均值填充平均值</span></span><br><span class="line"><span class="comment"># iris没有缺失值数据行,这里添加一行缺失值</span></span><br><span class="line">x_missing = np.append(iris.data, [[<span class="string">"Nan"</span>, <span class="string">"Nan"</span>, <span class="string">"Nan"</span>, <span class="string">"Nan"</span>]], axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># print(x_missing[-1])</span></span><br><span class="line">imp = SimpleImputer(missing_values=np.nan, strategy=<span class="string">'mean'</span>)</span><br><span class="line">x_fill_nan = imp.fit_transform(x_missing)</span><br><span class="line"><span class="comment"># print(x_fill_nan[-1])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多项式变换(对行变量处理)</span></span><br><span class="line"><span class="comment"># 假如一个输入样本是２维的,形式如[a,b] ,则二阶多项式的特征集包括(a+b)(a+b)合并后的每一项</span></span><br><span class="line"><span class="comment"># iris数据集有四项特征,(a+b+c+d)(a+b+c+d)=a^2+2ab+2ac+2ad+b^2+2bc+2bd+c^2+2cd+d^2,再加上本身a,b,c,d四项特征和1这项特征,共15项</span></span><br><span class="line"><span class="comment"># interaction_only=False表示有a平方和b平方这样的项,include_bias=True表示有第一项1</span></span><br><span class="line">pf = PolynomialFeatures(<span class="number">2</span>, interaction_only=<span class="literal">False</span>, include_bias=<span class="literal">True</span>)</span><br><span class="line">x_pf = pf.fit_transform(x)</span><br><span class="line"><span class="comment"># print(x_pf[0])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义函数变换特征,以log函数为例,loglp即ln(x+1)</span></span><br><span class="line">ft = FunctionTransformer(np.log1p, validate=<span class="literal">False</span>)</span><br><span class="line">x_log = ft.fit_transform(x)</span><br><span class="line"><span class="comment"># print(x_log[0])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Filter)过滤法主要思想:</span></span><br><span class="line"><span class="comment"># 按照发散性或者相关性对各个特征进行对每一维的特征“打分”</span></span><br><span class="line"><span class="comment"># 即给每一维的特征赋予权重，这样的权重就代表着该维特征的重要性，设定阈值或者待选择阈值的个数，选择特征</span></span><br><span class="line"><span class="comment"># (Filter)过滤法包括方差选择特征法、卡方检验法、皮尔森相关系数法</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用方差选择特征,选择方差大于阈值的特征</span></span><br><span class="line">vt = VarianceThreshold(threshold=<span class="number">0.3</span>)</span><br><span class="line">x_variance_select = vt.fit_transform(x_train)</span><br><span class="line"><span class="comment"># print(x_variance_select[0])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过卡方检验,选择K个与标签最相关的特征,k即选取得分最大的前topk个特征</span></span><br><span class="line"><span class="comment"># 卡方检验用来找出特征对类别的相关性</span></span><br><span class="line"><span class="comment"># 我们总是假设H0:观察频数与期望频数没有差别,即某个特征对类别的频数没有影响</span></span><br><span class="line"><span class="comment"># 举例:</span></span><br><span class="line"><span class="comment">#                  体重下降 体重未下降 合计 体重下降率</span></span><br><span class="line"><span class="comment"># 吃晚饭组          123     467        590  20.85%</span></span><br><span class="line"><span class="comment"># 不吃晚饭组        45      106        151  29.80%</span></span><br><span class="line"><span class="comment"># 合计              168     573        741  22.67%</span></span><br><span class="line"><span class="comment"># 这里类别有两个,即体重下降和体重不下降;特征也有两个,即吃晚饭和不吃晚饭</span></span><br><span class="line"><span class="comment"># 建立假设检验</span></span><br><span class="line"><span class="comment"># H0：r1＝r2，不吃晚饭对体重下降没有影响，即吃不吃晚饭的体重下降率相等；</span></span><br><span class="line"><span class="comment"># H1：r1≠r2，不吃晚饭对体重下降有显著影响，即吃不吃晚饭的体重下降率不相等,α=0.05</span></span><br><span class="line"><span class="comment"># 如果不吃饭玩对体重下降没有影响,即假设H0,查表知道总体样本中体重下降率为22.67%</span></span><br><span class="line"><span class="comment"># 则理论值为</span></span><br><span class="line"><span class="comment">#                  体重下降 体重未下降 合计</span></span><br><span class="line"><span class="comment"># 吃晚饭组         133.765  456.234    590</span></span><br><span class="line"><span class="comment"># 不吃晚饭组       34.2348  116.765    151</span></span><br><span class="line"><span class="comment"># 合计             168      573        741</span></span><br><span class="line"><span class="comment"># 如果不吃饭玩与体重下降真的是独立无关的,那么四格表里的理论值和实际值差别应该会很小</span></span><br><span class="line"><span class="comment"># 卡方检验计算公式:x^2=对所有类别求和:(该类别样本数-该类别理论上的样本数)^2/该类别理论上的样本数</span></span><br><span class="line"><span class="comment"># 于是上面例子的卡方值为:</span></span><br><span class="line"><span class="comment"># 卡方值=5.498</span></span><br><span class="line"><span class="comment"># 求自由度:理论频次项加观察频次项=2,类别数k,自由度=(2-1)*(k-1),显然上面例子的自由度为1</span></span><br><span class="line"><span class="comment"># 根据自由度和分位数查卡方分布表,自由度为1,分位数为α=0.05,查表得到值3.84</span></span><br><span class="line"><span class="comment"># 而计算得卡方值5.498＞3.84,查表可看出α在0.02到0.01之间,所以能够以95%的概率拒绝H0假设(否定HO假设),可以认为不吃晚饭对体重下降有显著影响。</span></span><br><span class="line">sk = SelectKBest(chi2, k=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># chi2,卡方统计量，X中特征取值必须非负,卡方检验用来测度随机变量之间的依赖关系。</span></span><br><span class="line"><span class="comment"># 通过卡方检验得到的特征之间是最可能独立的随机变量(卡方值越小变量越独立),因此这些特征的区分度很高。</span></span><br><span class="line">x_kbest_select = sk.fit_transform(x_train, y_train)</span><br><span class="line"><span class="comment"># 每个特征的卡方值,卡方值越小得分越高</span></span><br><span class="line"><span class="comment"># print(sk.pvalues_)</span></span><br><span class="line"><span class="comment"># 每个特征的得分</span></span><br><span class="line"><span class="comment"># print(sk.scores_)</span></span><br><span class="line"><span class="comment"># print(x_kbest_select[0])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pearson相关系数(适用于回归问题)</span></span><br><span class="line"><span class="comment"># 相关系数是两个随机变量的协方差与两个随机变量的标准差之积的比值</span></span><br><span class="line"><span class="comment"># 皮尔森相关系数方法衡量的是变量之间的线性相关性,结果的取值区间为[-1，1]，-1表示完全的负相关，+1表示完全的正相关，0表示没有线性相关。</span></span><br><span class="line"><span class="comment"># Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感</span></span><br><span class="line"><span class="comment"># 如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0</span></span><br><span class="line"><span class="comment"># Scipy的pearsonr方法能够同时计算相关系数和p-value</span></span><br><span class="line"><span class="comment"># 固定种子</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">x = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, <span class="number">300</span>)</span><br><span class="line"><span class="comment"># pearsonr(x, y)的输入为特征矩阵和目标向量</span></span><br><span class="line"><span class="comment"># np.random.normal(0, 1, 100) 创建100个均值为0，方差为1的高斯随机数</span></span><br><span class="line"><span class="comment"># 输出为二元组(sorce, p-value)的数组</span></span><br><span class="line">print(<span class="string">"低噪音时:"</span>, pearsonr(x, x + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, <span class="number">300</span>)))</span><br><span class="line">print(<span class="string">"高噪音时:"</span>, pearsonr(x, x + np.random.normal(<span class="number">0</span>, <span class="number">10</span>, <span class="number">300</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Wrapper)包装法选择特征主要思想:</span></span><br><span class="line"><span class="comment"># 根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征。</span></span><br><span class="line"><span class="comment"># 也可以将特征子集的选择看作是一个搜索寻优问题，生成不同的组合，对组合进行评价，再与其他的组合进行比较。</span></span><br><span class="line"><span class="comment"># 这样就将子集的选择看作是一个是一个优化问题</span></span><br><span class="line"><span class="comment"># (Wrapper)包装法包括递归特征消除法</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 递归特征消除法来选择特征，这里选择逻辑回归作为基模型，n_features_to_select为保留的特征个数</span></span><br><span class="line"><span class="comment"># 递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练</span></span><br><span class="line"><span class="comment"># 以经典的SVM-RFE算法中来讨论此算法。首先，在每一轮训练过程中，会选择所有特征来进行训练，继而得到了分类的超平面w*x+b=0</span></span><br><span class="line"><span class="comment"># 如果有n个特征，那么SVM-RFE会选择出w中分量的平方值最小的那个序号i对应的特征，将其删除</span></span><br><span class="line"><span class="comment"># 在第二类的时候，特征数就剩下了n-1个，继续用这n-1个特征和输出值来训练SVM</span></span><br><span class="line"><span class="comment"># 同样的，继续去掉w中分量的平方值最小所对应的特征。以此类推，直到剩下的特征数满足我们的要求为止。</span></span><br><span class="line"><span class="comment"># 具体到SVM在sklearn中应用时，可以通过学习器返回的coef_属性或feature_importance_属性来获得每个特征的重要程度</span></span><br><span class="line"><span class="comment"># 然后从当前的特征集合中移除不重要的特征。在特征集合上不断重复上述过程，直到最终达到所需要的特征数量为止。</span></span><br><span class="line">rfe = RFE(estimator=LogisticRegression(multi_class=<span class="string">"auto"</span>, solver=<span class="string">"liblinear"</span>), n_features_to_select=<span class="number">2</span>)</span><br><span class="line">x_rfe = rfe.fit_transform(x_train, y_train)</span><br><span class="line"><span class="comment"># print(x_rfe[0])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Embedded)嵌入法选择特征的主要思想:</span></span><br><span class="line"><span class="comment"># 使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。</span></span><br><span class="line"><span class="comment"># 类似于Filter方法，但是是通过训练来确定特征的优劣,挑选出那些对模型的训练有重要意义的属性。</span></span><br><span class="line"><span class="comment"># (Embedded)嵌入法包括基于惩罚项的特征选择法和基于树模型的特征选择法</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于惩罚项的特征选择法,这里选择带L1惩罚项的逻辑回归作为基模型,可以输入一个阈值过滤不重要的特征</span></span><br><span class="line"><span class="comment"># 如果使用参数惩罚设置为L1，则使用的阈值为1e-5，否则默认使用mean</span></span><br><span class="line">sf_lr = SelectFromModel(LogisticRegression(penalty=<span class="string">"l1"</span>, C=<span class="number">0.1</span>, multi_class=<span class="string">"auto"</span>, solver=<span class="string">"liblinear"</span>), <span class="number">0.3</span>)</span><br><span class="line">x_sf_lr = sf_lr.fit_transform(x_train, y_train)</span><br><span class="line"><span class="comment"># print(x_sf_lr[0])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于树模型的特征选择法,这里选择GBDT模型作为基模型</span></span><br><span class="line"><span class="comment"># GradientBoostingClassifier为GBDT的分类器</span></span><br><span class="line">sf_gbdt = SelectFromModel(GradientBoostingClassifier())</span><br><span class="line">x_sf_gbdt = sf_gbdt.fit_transform(x_train, y_train)</span><br><span class="line"><span class="comment"># print(x_sf_gbdt[0])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 两种降维方法PCA和LDA。PCA是一种无监督的数据降维方法，LDA是一种有监督的数据降维方法</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># PCA降维,参数n_components为降维后的维数</span></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">x_pca = pca.fit_transform(x_train)</span><br><span class="line"><span class="comment"># 打印保留的维度的方差</span></span><br><span class="line"><span class="comment"># print(pca.explained_variance_ratio_)</span></span><br><span class="line"><span class="comment"># print(x_pca[0])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性判别分析(LDA)降维,参数n_components为降维后的维数</span></span><br><span class="line"><span class="comment"># PCA是将数据投影到方差最大的几个相互正交的方向上，以期待保留最多的样本信息</span></span><br><span class="line"><span class="comment"># LDA希望投影后相同类别的组内方差小,而组间方差大,使投影后使得同类样本尽可能近,不同类样本尽可能远</span></span><br><span class="line">lda = LDA(n_components=<span class="number">2</span>)</span><br><span class="line">x_lda = lda.fit_transform(x_train, y_train)</span><br><span class="line"><span class="comment"># print(x_lda[0])</span></span><br></pre></td></tr></table></figure>

<h1 id="特征工程数据可视化"><a href="#特征工程数据可视化" class="headerlink" title="特征工程数据可视化"></a>特征工程数据可视化</h1><p>我们使用kaggle著名的House Prices: Advanced Regression Techniques案例数据集。数据集下载地址:<a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" target="_blank" rel="noopener">https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data</a> 。<br><strong>代码实现如下:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"></span><br><span class="line">df_train = pd.read_csv(<span class="string">'./house_prices/train.csv'</span>)</span><br><span class="line">df_test = pd.read_csv(<span class="string">'./house_prices/test.csv'</span>)</span><br><span class="line"><span class="comment"># print(df_train.columns)</span></span><br><span class="line">print(df_train.shape, df_test.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小价格大于0,数据均有效</span></span><br><span class="line"><span class="comment"># print(df_train['SalePrice'].describe())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 价格分布向左偏移,显然不满足正态分布</span></span><br><span class="line"><span class="comment"># sns.distplot(df_train['SalePrice'])</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 求价格的偏度和峰度</span></span><br><span class="line"><span class="comment"># 偏度是样本的三阶标准化矩,即((X-μ)/σ)^3的期望</span></span><br><span class="line"><span class="comment"># 峰度是四阶累积量除以二阶累积量的平方,即(X-μ)^4的期望为分子,((X-μ)^2的期望)^2为分母</span></span><br><span class="line"><span class="comment"># print("Skewness:&#123;&#125;,Kurtosis:&#123;&#125;".format(df_train['SalePrice'].skew(), df_train['SalePrice'].kurt()))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 'GrLivArea'与SalePrice两变量的图像,可以看出这两个变量有明显的线性关系</span></span><br><span class="line"><span class="comment"># data = pd.concat([df_train['SalePrice'], df_train['GrLivArea']], axis=1)</span></span><br><span class="line"><span class="comment"># data.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0, 800000))</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 'TotalBsmtSF'与SalePrice两变量的图像,显然这两个变量也有明显的线性关系</span></span><br><span class="line"><span class="comment"># data = pd.concat([df_train['SalePrice'], df_train['TotalBsmtSF']], axis=1)</span></span><br><span class="line"><span class="comment"># data.plot.scatter(x='TotalBsmtSF', y='SalePrice', ylim=(0, 800000))</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 'OverallQual'与SalePrice两变量的图像,销售价格随着商品整体质量的提高而升高</span></span><br><span class="line"><span class="comment"># data = pd.concat([df_train['SalePrice'], df_train['OverallQual']], axis=1)</span></span><br><span class="line"><span class="comment"># f, ax = plt.subplots(figsize=(8, 6))</span></span><br><span class="line"><span class="comment"># fig = sns.boxplot(x='OverallQual', y="SalePrice", data=data)</span></span><br><span class="line"><span class="comment"># fig.axis(ymin=0, ymax=800000)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 'YearBuilt'与SalePrice两变量的图像,显然人们倾向于在新物品上花更多的钱</span></span><br><span class="line"><span class="comment"># data = pd.concat([df_train['SalePrice'], df_train['YearBuilt']], axis=1)</span></span><br><span class="line"><span class="comment"># f_2, ax_2 = plt.subplots(figsize=(16, 8))</span></span><br><span class="line"><span class="comment"># fig_2 = sns.boxplot(x='YearBuilt', y="SalePrice", data=data)</span></span><br><span class="line"><span class="comment"># fig_2.axis(ymin=0, ymax=800000)</span></span><br><span class="line"><span class="comment"># plt.xticks(rotation=90)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 上面我们只是根据这几个特征的含义推断出它们之间可能有关联,通过画图分析我们证实了它们的关系</span></span><br><span class="line"><span class="comment"># # 现在我们要针对有所特征画热力图形式的矩阵来判断任意两两特征之间的相关性</span></span><br><span class="line"><span class="comment"># # data.corr()即相关系数矩阵，给出了任意两个变量之间的相关系数</span></span><br><span class="line"><span class="comment"># # 颜色越浅代表相关性越强，可以看到'TotalBsmtSF'和'1stFlrSF'特征的相关性非常之强</span></span><br><span class="line"><span class="comment"># corrmat = df_train.corr()</span></span><br><span class="line"><span class="comment"># f, ax = plt.subplots(figsize=(12, 9))</span></span><br><span class="line"><span class="comment"># # vmax、vmin即热力图颜色取值的最大值和最小值,默认会从data中推导</span></span><br><span class="line"><span class="comment"># # square=True会将单元格设为正方形</span></span><br><span class="line"><span class="comment"># sns.heatmap(corrmat, square=True, ax=ax, cmap='Blues')</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 打印相关系数矩阵,只显示大于0.5的值的项,其他均为NaN</span></span><br><span class="line"><span class="comment"># print(df_train.corr()[df_train.corr() &gt; 0.5])</span></span><br><span class="line"><span class="comment"># # 查看SalePrice对其他变量的相关系数,按从大到小排序</span></span><br><span class="line"><span class="comment"># corr = df_train.corr()['SalePrice']</span></span><br><span class="line"><span class="comment"># print(corr[np.argsort(corr, axis=0)[::-1]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 画出指定特征个数的热力图皮尔森相关系数矩阵,这里指定k=10</span></span><br><span class="line"><span class="comment"># k = 10</span></span><br><span class="line"><span class="comment"># corrmat = df_train.corr()</span></span><br><span class="line"><span class="comment"># # nlargest()的第一个参数就是截取的行数,这里就是从相关系数矩阵中截取相关性最高的前10个特征,第二个参数就是依据的列名</span></span><br><span class="line"><span class="comment"># cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index</span></span><br><span class="line"><span class="comment"># # np.corrcoef()计算皮尔逊积矩相关系数</span></span><br><span class="line"><span class="comment"># cm = np.corrcoef(df_train[cols].values.T)</span></span><br><span class="line"><span class="comment"># # 设置字体大小</span></span><br><span class="line"><span class="comment"># sns.set(font_scale=1)</span></span><br><span class="line"><span class="comment"># # annot=True在每个方格中写入数据，square=True会将单元格设为正方形</span></span><br><span class="line"><span class="comment"># # cbar是否在热力图侧边绘制颜色刻度条，默认值是True</span></span><br><span class="line"><span class="comment"># sns.heatmap(cm, annot=True, square=True, fmt='.2f', annot_kws=&#123;'size': 10&#125;, yticklabels=cols.values,</span></span><br><span class="line"><span class="comment">#             xticklabels=cols.values, cmap='Blues')</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 我们还可以一次性画出多个变量中两两变量之间的散点图来查看它们之间有无线性关系</span></span><br><span class="line"><span class="comment"># cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']</span></span><br><span class="line"><span class="comment"># sns.pairplot(df_train[cols], height=2)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 我们也可以一次性对多个变量中画每个变量的数据的直方图来查看数据分布</span></span><br><span class="line"><span class="comment"># # bins指一张图中有几个条形</span></span><br><span class="line"><span class="comment"># df_train.hist(bins=20, figsize=(20, 15))</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缺失值占总样本数百分比统计</span></span><br><span class="line">total = df_train.isnull().sum().sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 缺失值数量/总样本数量,ascending=False降序排列</span></span><br><span class="line">percent = (df_train.isnull().sum() / df_train.isnull().count()).sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line">missing_data = pd.concat([total, percent], axis=<span class="number">1</span>, keys=[<span class="string">'Total'</span>, <span class="string">'Percent'</span>])</span><br><span class="line"><span class="comment"># print(missing_data)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们将删除缺失值超过15%的变量,如'PoolQC', 'MiscFeature', 'Alley','Fence','FireplaceQu','LotFrontage'</span></span><br><span class="line"><span class="comment">#  'MasVnrArea'和'MasVnrType'特征与'YearBuilt'和'OverallQual'有很强的相关性,因此我们删除'MasVnrArea'和'MasVnrType'</span></span><br><span class="line"><span class="comment"># Garage的几个变量缺失的都是同样行的数据,因为关于车库的最重要信息是GarageCars,所以我们删除这几个特征</span></span><br><span class="line"><span class="comment"># 同理,我们删除Bsmt几个特征</span></span><br><span class="line"><span class="comment"># Electrical特征只有一项缺失值,我们删除有缺失值这一行数据即可</span></span><br><span class="line">df_train = df_train.drop((missing_data[missing_data[<span class="string">'Total'</span>] &gt; <span class="number">1</span>]).index, <span class="number">1</span>)</span><br><span class="line">df_train = df_train.drop(df_train.loc[df_train[<span class="string">'Electrical'</span>].isnull()].index)</span><br><span class="line"><span class="comment"># 检查还有无缺失值数据</span></span><br><span class="line"><span class="comment"># print(df_train.isnull().sum().max())</span></span><br><span class="line">df_train = pd.get_dummies(df_train, drop_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 下面分析异常值,如SalePrice不应该小于0</span></span><br><span class="line"><span class="comment"># # 先把数据进行标准化</span></span><br><span class="line"><span class="comment"># # 新建一列存放标准化后的SalePrice</span></span><br><span class="line"><span class="comment"># # 提取最低的10个数和最高的10个数,可以发现最低的数它们的距离都较近，离0也较近；而最高的10个数里有两个大于7的值，疑似离散点</span></span><br><span class="line"><span class="comment"># sale_price_scaled = StandardScaler().fit_transform((df_train['SalePrice'][:, np.newaxis]).astype(float))</span></span><br><span class="line"><span class="comment"># # y=x.argsort()函数是将x中的元素从小到大排列，提取其对应的index(索引)，然后输出到y</span></span><br><span class="line"><span class="comment"># low_range = sale_price_scaled[sale_price_scaled[:, 0].argsort()][:10]</span></span><br><span class="line"><span class="comment"># high_range = sale_price_scaled[sale_price_scaled[:, 0].argsort()][-10:]</span></span><br><span class="line"><span class="comment"># # print(low_range)</span></span><br><span class="line"><span class="comment"># # print(high_range)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 前面我们已经进行过双变量分析,比如下面两个变量</span></span><br><span class="line"><span class="comment"># # 右下角有两个GrLivArea值很大的点,是异常点</span></span><br><span class="line"><span class="comment"># 右上角靠左也有两个点,这两点就是上面SalePrice中两个大于7的值,注意这两个点不是异常点,先保留，有待观察</span></span><br><span class="line"><span class="comment"># data = pd.concat([df_train['SalePrice'], df_train['GrLivArea']], axis=1)</span></span><br><span class="line"><span class="comment"># data.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0, 800000))</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除上面两个异常点</span></span><br><span class="line">df_train_outlier = df_train.sort_values(by=<span class="string">'GrLivArea'</span>, ascending=<span class="literal">False</span>)[:<span class="number">2</span>]</span><br><span class="line"><span class="comment"># print(df_train_outlier)</span></span><br><span class="line">df_train = df_train.drop(df_train[df_train[<span class="string">'Id'</span>] == <span class="number">1299</span>].index)</span><br><span class="line">df_train = df_train.drop(df_train[df_train[<span class="string">'Id'</span>] == <span class="number">524</span>].index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 再观察下面这两个变量,可以看到有一些TotalBsmtSF&gt;3000的离群点,不过我们先保留这些点,先不要删除它们</span></span><br><span class="line"><span class="comment"># data = pd.concat([df_train['SalePrice'], df_train['TotalBsmtSF']], axis=1)</span></span><br><span class="line"><span class="comment"># data.plot.scatter(x='TotalBsmtSF', y='SalePrice', ylim=(0, 800000))</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 下面探究变量是否服从正态分布,显然SalePrice偏离了正态分布的中心</span></span><br><span class="line"><span class="comment"># # displot()集合了matplotlib的hist()与核函数估计kdeplot的功能，增加了rugplot分布观测条显示与利用scipy库fit拟合参数分布的新用途</span></span><br><span class="line"><span class="comment"># # 核密度估计是在概率论中用来估计未知的密度函数</span></span><br><span class="line"><span class="comment"># # fit=norm即指图上黑色的标准正态分布曲线</span></span><br><span class="line"><span class="comment"># sns.distplot(df_train['SalePrice'], fit=norm)</span></span><br><span class="line"><span class="comment"># fig = plt.figure()</span></span><br><span class="line"><span class="comment"># # 计算概率图的分位数</span></span><br><span class="line"><span class="comment"># # plot如果给出,则根据给出的数据绘制分位数和最小二乘拟合</span></span><br><span class="line"><span class="comment"># res = stats.probplot(df_train['SalePrice'], plot=plt)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 我们把SalePrice值取log对数后再画图看看,现在SalePrice值看起来服从正态分布了</span></span><br><span class="line"><span class="comment"># df_train['SalePrice'] = np.log(df_train['SalePrice'])</span></span><br><span class="line"><span class="comment"># sns.distplot(df_train['SalePrice'], fit=norm)</span></span><br><span class="line"><span class="comment"># fig = plt.figure()</span></span><br><span class="line"><span class="comment"># stats.probplot(df_train['SalePrice'], plot=plt)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # GrLivArea的值也取log对数,这样它的值也从左偏离正态分布变得服从正态分布了</span></span><br><span class="line"><span class="comment"># df_train['GrLivArea'] = np.log(df_train['GrLivArea'])</span></span><br><span class="line"><span class="comment"># sns.distplot(df_train['GrLivArea'], fit=norm)</span></span><br><span class="line"><span class="comment"># fig = plt.figure()</span></span><br><span class="line"><span class="comment"># res = stats.probplot(df_train['GrLivArea'], plot=plt)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 一组随机变量具备同方差即指线性回归的最小二乘法的残值服从均值为0，方差为σ^2的正态分布，即其干扰项必须服从随机分布</span></span><br><span class="line"><span class="comment"># # 测试两个度量变量的同方差的最佳方法是图形化，先对特征'GrLivArea'和'SalePrice'</span></span><br><span class="line"><span class="comment"># plt.scatter(df_train['GrLivArea'], df_train['SalePrice'])</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 再观察'TotalBsmtSF'(只看大于0的值)和'SalePrice',注意这里TotalBsmtSF的非零值已经取过对数</span></span><br><span class="line"><span class="comment"># plt.scatter(df_train[df_train['TotalBsmtSF'] &gt; 0]['TotalBsmtSF'], df_train[df_train['TotalBsmtSF'] &gt; 0]['SalePrice'])</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br></pre></td></tr></table></figure>
            </div>
            <hr/>

            
            <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.88rem;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff;
        background-color: #22AB38;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff;
        background-color: #019FE8;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a class="reward-link btn-floating btn-large waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close"><i class="fa fa-close"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs">
                        <li class="tab wechat-tab waves-effect waves-light"><a class="active" href="#wechat">微信</a></li>
                        <li class="tab alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                    </ul>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('#reward .reward-link').on('click', function () {
            $('#rewardModal').openModal();
        });

        $('#rewardModal .close').on('click', function () {
            $('#rewardModal').closeModal();
        });
    });
</script>
            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone" data-wechat-qrcode-helper="<p>微信里点“发现”->“扫一扫”二维码便可查看分享。</p>"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            <div class="reprint">
                <p>
                    <span class="reprint-tip">
                        <i class="fa fa-exclamation-circle"></i>&nbsp;&nbsp;Reprint please specify:
                    </span>
                    <a href="https://wyg1996.cn" class="b-link-green">鱼缸屋</a>
                    <i class="fa fa-angle-right fa-lg fa-fw text-color"></i>
                    <a href="/2019/04/06/特征工程sklearn基础实践、特征工程数据可视化/" class="b-link-green">特征工程sklearn基础实践、特征工程数据可视化</a>
                </p>
            </div>
        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '',
        clientSecret: '',
        repo: '',
        owner: '',
        admin: null,
        id: '2019-04-06T12-16-47',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    
        <link rel="stylesheet" href="/libs/gitment/gitment-default.css">
<link rel="stylesheet" href="/css/gitment.css">

<div class="gitment-card card" data-aos="fade-up">
    <div id="gitment-content" class="card-content"></div>
</div>

<script src="/libs/gitment/gitment.js"></script>
<script>
var gitment = new Gitment({
    id: 'Sat Apr 06 2019 12:16:47 GMT+0800',
    owner: '',
    repo: '',
    oauth: {
        client_id: '',
        client_secret: ''
    }
});

gitment.render('gitment-content');
</script>
    

    
        <div class="disqus-card card" data-aos="fade-up">
    <div id="disqus_thread" class="card-content">
        <noscript>Please enable JavaScript to view the
            <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
    </div>
</div>

<script type="text/javascript">
    disqus_config = function () {
        this.page.url = 'https://wyg1996.cn/2019/04/06/特征工程sklearn基础实践、特征工程数据可视化/';
        this.page.identifier = '/2019/04/06/特征工程sklearn基础实践、特征工程数据可视化/';
        this.page.title = '特征工程sklearn基础实践、特征工程数据可视化';
    };
    let disqus_shortname = '';

    (function () { // DON'T EDIT BELOW THIS LINE
        let d = document, s = d.createElement('script');
        // 如：s.src = 'https://blinkfox.disqus.com/embed.js';
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
    

    
    <div class="livere-card card" data-aos="fade-up">
    <!-- 来必力City版安装代码 -->
    <div id="lv-container" class="card-content" data-id="city" data-uid="">
        <script type="text/javascript">
            (function (d, s) {
                let j, e = d.getElementsByTagName(s)[0];
                if (typeof LivereTower === 'function') {
                    return;
                }

                j = d.createElement(s);
                j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
                j.async = true;

                e.parentNode.insertBefore(j, e);
            })(document, 'script');
        </script>
        <noscript>为正常使用来必力评论功能请激活JavaScript。</noscript>
    </div>
    <!-- City版安装代码已完成 -->
</div>
    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2019/04/07/771-Jewels and Stones/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/3.jpg" class="responsive-img" alt="771-Jewels and Stones">
                        
                        <span class="card-title">771-Jewels and Stones</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">题目You’re given strings J representing the types of stones that are jewels, and S representing the stones you have.  Each</div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="fa fa-clock-o fa-fw icon-date"></i>2019-04-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Leetcode/" class="post-category" target="_blank">
                                    Leetcode
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Leetcode/" target="_blank">
                        <span class="chip bg-color">Leetcode</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2019/04/05/特征工程实践：泰坦尼克号幸存者预测/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/23.jpg" class="responsive-img" alt="特征工程实践:泰坦尼克号幸存者预测">
                        
                        <span class="card-title">特征工程实践:泰坦尼克号幸存者预测</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">泰坦尼克号幸存者预测数据集下载地址:https://www.kaggle.com/c/titanic/data 。本案例主要展示特征工程对数据集的处理方法，模型只选择了简单的lr模型，最后得分并不高。
1234567891011121314</div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2019-04-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/特征工程/" class="post-category" target="_blank">
                                    特征工程
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/特征工程/" target="_blank">
                        <span class="chip bg-color">特征工程</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + 'From: 鱼缸屋<br />'
            + 'Author: ygwu<br />'
            + 'Link: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>

    </div>
    <div class="col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });
    });
</script>
    

</main>


<footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            &copy;<a href="mailto:ygwu@mail.ustc.edu.cn" target="_blank">鱼缸屋</a>
            

            
                &nbsp;<i class="fa fa-area-chart"></i>&nbsp;站点总字数:&nbsp;
                <span class="white-color">393.7k</span>
            

            
			
                <br>
                
                <span id="busuanzi_container_site_pv">
                    <i class="fa fa-heart-o"></i>
                    本站总访问量 <span id="busuanzi_value_site_pv" class="white-color"></span>
                </span>
                
                
                <span id="busuanzi_container_site_uv">
                    <i class="fa fa-users"></i>
                    次,&nbsp;访客数 <span id="busuanzi_value_site_uv" class="white-color"></span> 人.
                </span>
                
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/wygny" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:ygwu@mail.ustc.edu.cn" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=321699849" class="tooltipped" data-tooltip="QQ联系我: 321699849" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>



    <a href="https://www.baidu.com" class="tooltipped" target="_blank" data-tooltip="访问百度" data-position="top" data-delay="50">
        <i class="fa fa-paw"></i>
</a>



    <a href="https://www.google.com" class="tooltipped" target="_blank" data-tooltip="访问谷歌" data-position="top" data-delay="50">
        <i class="fa fa-google"></i>
</a>



    <a href="https://www.bilibili.com" class="tooltipped" target="_blank" data-tooltip="访问哔哩哔哩" data-position="top" data-delay="50">
        <i class="fa fa-bullseye"></i>
</a>



    <a href="https://blog.csdn.net/zgcr654321" class="tooltipped" target="_blank" data-tooltip="访问我的CSDN博客" data-position="top" data-delay="50">
        <i class="fa fa-copyright"></i>
</a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>




</div>
    </div>
</footer>

<div class="progress-bar"></div>


<!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input" autofocus="">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
</script>
<!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


<script src="/libs/materialize/js/materialize.min.js"></script>
<script src="/libs/masonry/masonry.pkgd.min.js"></script>
<script src="/libs/aos/aos.js"></script>
<script src="/libs/scrollprogress/scrollProgress.min.js"></script>
<script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
<script src="/js/matery.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->



    <script src="/libs/others/clicklove.js"></script>


    <script async src="/libs/others/busuanzi.pure.mini.js"></script>


</body>
</html>