<!DOCTYPE HTML>
<html lang="zh_CN">


<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="keywords" content="CUDA矩阵计算原理和方法, 鱼缸屋">
    <meta name="description" content="基本概念主机（host）将CPU及系统的内存（内存条）称为主机。
设备（device）将GPU及GPU本身的显示内存称为设备。
流式处理器（SP）流处理器SP（streaming processor,也叫CUDA core）是最基本的处理单">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>CUDA矩阵计算原理和方法 | 鱼缸屋</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/css/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    <style type="text/css">
        
    </style>

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
</head>


<body>

<header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="container">
            <div class="nav-wrapper">
                <div class="brand-logo">
                    <a href="/" class="waves-effect waves-light">
                        
                        <img src="/medias/logo.png" class="logo-img hide-on-small-only">
                        
                        <span class="logo-span">鱼缸屋</span>
                    </a>
                </div>
                

<a href="#" data-activates="mobile-nav" class="button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-home"></i>
            
            <span>首页</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>标签</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-bookmark"></i>
            
            <span>分类</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-archive"></i>
            
            <span>归档</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <i class="fa fa-user-circle-o"></i>
            
            <span>关于</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/friends" class="waves-effect waves-light">
            
            <i class="fa fa-address-book"></i>
            
            <span>友情链接</span>
        </a>
    </li>
    
    <li>
        <a id="toggleSearch" class="waves-effect waves-light">
            <i id="searchIcon" class="mdi-action-search" title="Search"></i>
        </a>
    </li>

</ul>

<div class="side-nav" id="mobile-nav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">鱼缸屋</div>
        <div class="logo-desc">
            
            海纳百川，有容乃大，壁立千仞，无欲则刚
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-home"></i>
                
                首页
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                标签
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-bookmark"></i>
                
                分类
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-archive"></i>
                
                归档
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-user-circle-o"></i>
                
                关于
            </a>
        </li>
        
        <li>
            <a href="/friends" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-address-book"></i>
                
                友情链接
            </a>
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/WYGNG" class="waves-effect waves-light" target="_blank">
                <i class="fa fa-github-square fa-fw"></i>来github一起玩吧！
            </a>
        </li>
        
    </ul>

    <div class="social-link">
    <a href="https://github.com/wygny" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:ygwu@mail.ustc.edu.cn" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=321699849" class="tooltipped" data-tooltip="QQ联系我: 321699849" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>



    <a href="https://www.baidu.com" class="tooltipped" target="_blank" data-tooltip="访问百度" data-position="top" data-delay="50">
        <i class="fa fa-paw"></i>
</a>



    <a href="https://www.google.com" class="tooltipped" target="_blank" data-tooltip="访问谷歌" data-position="top" data-delay="50">
        <i class="fa fa-google"></i>
</a>



    <a href="https://www.bilibili.com" class="tooltipped" target="_blank" data-tooltip="访问哔哩哔哩" data-position="top" data-delay="50">
        <i class="fa fa-bullseye"></i>
</a>



    <a href="https://blog.csdn.net/zgcr654321" class="tooltipped" target="_blank" data-tooltip="访问我的CSDN博客" data-position="top" data-delay="50">
        <i class="fa fa-copyright"></i>
</a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>




</div>
</div>

            </div>
        </div>

        
        <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/WYGNG" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="来github一起玩吧！" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>
</header>





<div class="bg-cover post-cover" style="background-image: url('/medias/featureimages/0.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        CUDA矩阵计算原理和方法
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }
</style>
<div class="row">
    <div class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/CUDA编程/" target="_blank">
                                <span class="chip bg-color">CUDA编程</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/CUDA编程/" class="post-category" target="_blank">
                                CUDA编程
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2019-05-15
                </div>

                
                    
                    <div class="info-break-policy">
                        <i class="fa fa-file-word-o fa-fw"></i>Word Count:&nbsp;&nbsp;
                        6.2k
                    </div>
                    

                    
                    <div class="info-break-policy">
                        <i class="fa fa-clock-o fa-fw"></i>Read Times:&nbsp;&nbsp;
                        25 Min
                    </div>
                    
                
				
				
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="fa fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><h2 id="主机（host）"><a href="#主机（host）" class="headerlink" title="主机（host）"></a>主机（host）</h2><p>将CPU及系统的内存（内存条）称为主机。</p>
<h2 id="设备（device）"><a href="#设备（device）" class="headerlink" title="设备（device）"></a>设备（device）</h2><p>将GPU及GPU本身的显示内存称为设备。</p>
<h2 id="流式处理器（SP）"><a href="#流式处理器（SP）" class="headerlink" title="流式处理器（SP）"></a>流式处理器（SP）</h2><p>流处理器SP（streaming processor,也叫CUDA core）是最基本的处理单元，最后具体的指令和任务都是在SP上处理的。GPU进行并行计算，也就是很多个SP同时做处理。</p>
<h2 id="流式多处理器（SM）"><a href="#流式多处理器（SM）" class="headerlink" title="流式多处理器（SM）"></a>流式多处理器（SM）</h2><p>多个SP加上其他的一些资源（warp，scheduler，register，shared memory等）组成一个SM（streaming multiprocessor）。也叫GPU大核。SM可以看做GPU的心脏（对比CPU核心），register和sharedmemory是SM的稀缺资源。CUDA将这些资源分配给所有驻留在SM中的threads。因此，这些有限的资源就使每个SM中active warps有非常严格的限制，也就限制了并行能力。<br>每个SM包含的SP数量依据GPU架构而不同，Fermi架构GF100是32个，GF10X是48个，Kepler架构都是192个，Maxwell都是128个。相同架构的GPU包含的SM数量则根据GPU的中高低端来定。在Maxwell架构中，Nvidia已经把SM改叫SMM。在软件逻辑上是所有SP是并行的，但是物理上并不是所有SP都能同时执行计算，因为有些会处于挂起，就绪等其他状态，这与GPU的线程调度有关。<br>GPU中每个sm都设计成支持数以百计的线程并行执行，并且每个GPU都包含了很多的SM，所以GPU支持成百上千的线程并行执行。当一个kernel启动后，thread会被分配到这些SM中执行。大量的thread可能会被分配到不同的SM，同一个block中的threads必然在同一个SM中并行（SIMT）执行。每个thread拥有它自己的程序计数器和状态寄存器，并且用该线程自己的数据执行指令，这就是所谓的Single Instruction Multiple Thread。<br>一个SP可以执行一个thread，但是实际上并不是所有的thread能够在同一时刻执行。Nvidia把32个threads组成一个warp，warp是调度和运行的基本单元。warp中所有threads并行的执行相同的指令。一个warp需要占用一个SM运行，多个warps需要轮流进入SM。由SM的硬件warp scheduler负责调度。目前每个warp包含32个threads（Nvidia保留修改数量的权利）。所以，一个GPU上resident thread最多只有SMxwarp个。 </p>
<h2 id="线程（Thread）"><a href="#线程（Thread）" class="headerlink" title="线程（Thread）"></a>线程（Thread）</h2><p>一般通过GPU的一个核进行处理。</p>
<h2 id="线程块（Block）"><a href="#线程块（Block）" class="headerlink" title="线程块（Block）"></a>线程块（Block）</h2><p>由多个线程组成（可以表示成一维，二维，三维）；<br>各block是并行执行的，block间无法通信，也没有执行顺序；<br>线程块的数量限制为不超过65535（硬件限制）。</p>
<h2 id="线程格（Grid）"><a href="#线程格（Grid）" class="headerlink" title="线程格（Grid）"></a>线程格（Grid）</h2><p>由多个线程块组成（可以表示成一维，二维，三维）。</p>
<h2 id="线程束（wrap）"><a href="#线程束（wrap）" class="headerlink" title="线程束（wrap）"></a>线程束（wrap）</h2><p>在CUDA架构中，线程束是指一个包含32个线程的集合，这个线程集合被“编织在一起”并且“步调一致”的形式执行。在程序中的每一行，线程束中的每个线程都将在不同数据上执行相同的命令。</p>
<h2 id="函数修饰符"><a href="#函数修饰符" class="headerlink" title="函数修饰符"></a>函数修饰符</h2><p><strong>在CUDA中，通过函数类型修饰符区分host和device上的函数:</strong><br>__global__：在device上执行，从host中调用（一些特定的GPU也可以从device上调用），返回类型必须是void，不支持可变参数参数，不能成为类成员函数。注意用__global__定义的kernel是异步的，这意味着host不会等待kernel执行完就执行下一步。<br>__device__：在device上执行，单仅可以从device中调用，不可以和__global__同时用。<br>__host__：在host上执行，仅可以从host上调用，一般省略不写，不可以和__global__同时用，但可和__device__，此时函数会在device和host都编译。</p>
<h1 id="GPU内存的分类"><a href="#GPU内存的分类" class="headerlink" title="GPU内存的分类"></a>GPU内存的分类</h1><h2 id="全局内存（Global-Memory）"><a href="#全局内存（Global-Memory）" class="headerlink" title="全局内存（Global Memory）"></a>全局内存（Global Memory）</h2><p>通俗意义上的设备内存。</p>
<h2 id="共享内存（Shared-Memory）"><a href="#共享内存（Shared-Memory）" class="headerlink" title="共享内存（Shared Memory）"></a>共享内存（Shared Memory）</h2><p>在设备内存上，以关键字__shared__添加到变量声明中。如__shared__ float cache[10]。对于GPU上启动的每个线程块，CUDA C编译器都将创建该共享变量的一个副本。线程块中的每个线程都共享这块内存，但线程却无法看到也不能修改其他线程块的变量副本。这样使得一个线程块中的多个线程能够在计算上通信和协作。</p>
<h2 id="常量内存（Constant-Memory）"><a href="#常量内存（Constant-Memory）" class="headerlink" title="常量内存（Constant Memory）"></a>常量内存（Constant Memory）</h2><p>在设备内存上，以关键字__constant__添加到变量声明中。如__constant__ float s[10];。常量内存定义的变量用于保存在核函数执行期间不会发生变化的数据。变量的访问限制为只读。NVIDIA硬件提供了64KB的常量内存。不再需要cudaMalloc()或者cudaFree(),而是在编译时，静态地分配空间。<br>常量内存其实只是全局内存的一种虚拟地址形式，并没有特殊保留的常量内存块。常量内存有两个特性，一个是高速缓存，另一个是它支持将单个值广播到线程束中的每个线程。当常量内存将数据分配或广播到线程束中的每个线程时（注意，实际上硬件会将单次内存读取操作广播到半个线程束），广播能够在单个周期内发生。当所有16个线程都读取相同地址时，这个功能可以极大提高性能，但当所有16个线程分别读取不同的地址时，它实际上会降低性能。如果半个线程束中的所有16个线程需要访问常量内存中的不同数据，那么这个16次不同的读取操作会被串行化，从而需要16倍的时间来发出请求。但如果从全局内存中读取，那么这些请求就会同时发出。这种情况下，从常量内存读取就会慢于从全局内存中读取。<br><strong>注意:</strong><br>当我们需要拷贝数据到常量内存中应该使用cudaMemcpyToSymbol()，而cudaMemcpy()会复制到全局内存。</p>
<h2 id="纹理内存（Texture-Memory）"><a href="#纹理内存（Texture-Memory）" class="headerlink" title="纹理内存（Texture Memory）"></a>纹理内存（Texture Memory）</h2><p>纹理内存是另一种类型的只读内存，在特定的访问模式中（以下例子并非这种特定的访问模式），纹理内存同样能够提升性能。纹理内存缓存在芯片上，因此在某些情况中，它能够减少对内存的请求并提供更高效的内存带宽。纹理缓存是专门为那些在内存访问模式中存在大量空间局部性(Spatial Locality)的图形应用程序而设计的。在某个计算应用程序中，这意味着一个线程读取的位置可能与邻近线程的读取位置“非常接近”。举个例子，一个2x2矩阵的四个元素在地址上不是连续的，但是在空间位置上是互相相邻的，纹理缓存就是专门为了加速这种访问模式而设计的。如果在这种情况中使用纹理内存而不是全局内存，那么将会获得性能的提升。<br>纹理变量（引用）必须声明为文件作用域内的全局变量，其形式分为一维纹理内存和二维纹理内存。<br><strong>一维纹理内存:</strong><br>用texture&lt;类型&gt;类型声明，如texture<float> texIn。通过cudaBindTexture()绑定到纹理内存中，通过tex1Dfetch()来读取纹理内存中的数据，通过cudaUnbindTexture()取消绑定纹理内存。<br><strong>二维纹理内存</strong><br>用texture&lt;类型,数字&gt;类型声明，如texture&lt;float，2&gt; texIn。通过cudaBindTexture2D()绑定到纹理内存中，通过tex2D()来读取纹理内存中的数据，通过cudaUnbindTexture()取消绑定纹理内存。</float></p>
<h2 id="固定内存"><a href="#固定内存" class="headerlink" title="固定内存"></a>固定内存</h2><p>在主机内存上，也称为页锁定内存或者不可分页内存，操作系统将不会对这块内存分页并交换到磁盘上，从而确保了该内存始终驻留在物理内存中。因此操作系统能够安全地使某个应用程序访问该内存的物理地址，因为这块内存将不会破坏或者重新定位。<br><strong>优点:</strong><br>固定内存可以提高访问速度。由于GPU知道主机内存的物理地址，因此可以通过DMA（直接内存访问，Direct Memory Access)技术来在GPU和主机之间复制数据。由于DMA在执行复制时无需CPU介入。因此DMA复制过程中使用固定内存是非常重要的。<br><strong>缺点:</strong><br>使用固定内存，将失去虚拟内存的所有功能，系统将更快的耗尽内存。<br><strong>注意:</strong><br>对cudaMemcpy()函数调用中的源内存或者目标内存，才使用固定内存，并且在不再需要使用它们时立即释放。固定内存通过cudaHostAlloc()函数来分配；通过cudaFreeHost()释放。我们只能以异步方式对固定内存进行复制操作。</p>
<h1 id="CUDA程序计算原理"><a href="#CUDA程序计算原理" class="headerlink" title="CUDA程序计算原理"></a>CUDA程序计算原理</h1><h2 id="CUDA程序执行过程"><a href="#CUDA程序执行过程" class="headerlink" title="CUDA程序执行过程"></a>CUDA程序执行过程</h2><p>分配host内存，并进行数据初始化；<br>分配device内存，并从host将数据拷贝到device上；<br>调用CUDA的核函数在device上完成指定的运算；<br>将device上的运算结果拷贝到host上；<br>释放device和host上分配的内存。</p>
<h2 id="核函数（kernel）与SM"><a href="#核函数（kernel）与SM" class="headerlink" title="核函数（kernel）与SM"></a>核函数（kernel）与SM</h2><p>kernel核函数是CUDA中一个重要的概念，kernel核函数是在device上线程中并行执行的函数，用__global__符号声明，在调用时需要用&lt;&lt;&lt;grid, block&gt;&gt;&gt;来指定一个kernel函数要执行的线程数量，在CUDA中，每个线程会分配一个唯一的线程号thread ID，这个ID值可以通过核函数的内置变量threadIdx来获得。<br>一个线程需要两个内置的坐标变量（blockIdx，threadIdx）来唯一标识，它们都是dim3类型变量，其中blockIdx指明线程所在grid中的位置，而threaIdx指明线程所在block中的位置。<br><strong>每个线程有自己的私有本地内存（Local Memory），每个线程块有包含共享内存（Shared Memory）,可以被线程块中所有线程共享，其生命周期与线程块一致。此外，所有的线程都可以访问全局内存（Global Memory），还可以访问一些只读内存块：常量内存（Constant Memory）和纹理内存（Texture Memory）。</strong><br><strong>一个kernel核函数在device上执行时实际上启动了很多线程，一个kernel所启动的所有线程称为一个线程格（grid），同一个线程格上的线程共享相同的全局内存空间；一个线程格又分为很多线程块（block），一个线程块里面包含很多线程。</strong><br>一个kernel核函数执行时会启动很多线程，这些线程是逻辑上并行的，但是在物理层上却不一定并行。但是一个GPU中存在很多CUDA核心（即SM），充分利用CUDA核心可以充分发挥GPU的并行计算能力。SM的核心组件包括CUDA核心，共享内存，寄存器等，SM可以并发地执行数百个线程，并发能力就取决于SM所拥有的资源数。<br><strong>当一个kernel核函数被执行时，它的网格块（grid）中的线程块（block）被分配到SM上，一个线程块（block）只能在一个SM上被调度。有时一个kernel核函数的各个线程块（block）被分配多个SM，那么网格块（grid）只是逻辑层，而SM才是执行的物理层。</strong><br>SM采用的是SIMT（Single-Instruction, Multiple-Thread，单指令多线程）架构，基本的执行单元是线程束（wraps），线程束包含32个线程，这些线程同时执行相同的指令，但是每个线程都包含自己的指令地址计数器和寄存器状态，也有自己独立的执行路径。所以尽管线程束（wraps）中的线程同时从同一程序地址执行，但是可能具有不同的行为，比如遇到了分支结构，一些线程可能进入这个分支，但是另外一些有可能不执行，它们只能死等，因为GPU规定线程束（wraps）中所有线程在同一周期执行相同的指令，线程束（wraps）分化会导致性能下降。<br>当线程块（block）被划分到某个SM上时，它将进一步划分为多个线程束（wraps），因为这才是SM的基本执行单元，但是一个SM同时并发的线程束（wraps）数是有限的。这是因为资源限制，SM要为每个线程块分配共享内存，而也要为每个线程束（wraps）中的线程分配独立的寄存器。所以SM的配置会影响其所支持的线程块和线程束并发数量。<br><strong>从逻辑上划分，一个网格块（grid）包含多个线程块（block），一个线程块（block）包含有多个线程（Threads）。但是一个kernel核函数的所有线程在物理层不一定是同时并发的。因此，kernel核函数的网格块（grid）和线程块（block）的配置不同，性能会出现差异。另外，由于SM的基本执行单元是包含32个线程的线程束，所以线程块（block）大小一般要设置为32的倍数。</strong></p>
<h2 id="核函数（kernel）的调用"><a href="#核函数（kernel）的调用" class="headerlink" title="核函数（kernel）的调用"></a>核函数（kernel）的调用</h2><p><strong>在VS2017的CUDA项目中启动kernel函数时要指定gridsize和blocksize，如:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> dim3 gridsize(2,2);</span><br><span class="line"> dim3 blocksize(4,4);</span><br><span class="line">Kernel &lt;&lt; &lt; gridSize, blocksize &gt;&gt; &gt; (A, B, C);</span><br></pre></td></tr></table></figure>

<p>这里的grid和block都是2D的。<br><strong>gridsize相当于是一个2x2的block，gridDim.x，gridDim.y，gridDim.z相当于这个dim3的x，y，z方向的维度，这里是2x2x1。序号从0到3，且是从上到下的顺序，即grid中的blockidx序号标注情况为:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0     2</span><br><span class="line">1     3</span><br></pre></td></tr></table></figure>

<p><strong>blocksize则是指block里面的线程(thread)的情况，blockDim.x，blockDim.y，blockDim.z相当于这个dim3的x，y，z方向的维度，这里是4x4x1.序号是0-15，即block中的threadidx序号标注情况为:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0      4       8      12 </span><br><span class="line">1       5       9       13</span><br><span class="line">2       6       10     14</span><br><span class="line">3       7       11      15</span><br></pre></td></tr></table></figure>

<p><strong>确定线程的global ID:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">int col = threadIdx.x + blockIdx.x * blockDim.x;</span><br></pre></td></tr></table></figure>

<p><strong>ThreadID是线性增长的，其目的是用于在硬件和软件上唯一标识每一个线程。CUDA程序中任何一个时刻，每一个线程的ThreadIdx都是特定唯一标识的。Threads的唯一标识ThreadIdx的表达方式随着grid，block的划分维度而不同</strong>。</p>
<h2 id="线程同步"><a href="#线程同步" class="headerlink" title="线程同步"></a>线程同步</h2><p>线程同步是针对同一个线程块（block）中的所有线程而言的，因为只有同一个线程块（block）中的线程才能在有效的机制中共同访问共享内存（Shared Memory）。由于每一个线程（Thread）的生命周期长度是不相同的，线程（Thread）对共享内存（Shared Memory）的操作可能会导致读写的不一致，因此需要线程的同步，从而保证该block中所有线程同时结束。**</p>
<h1 id="win10-VS2017-CUDA10-0项目配置"><a href="#win10-VS2017-CUDA10-0项目配置" class="headerlink" title="win10+VS2017+CUDA10.0项目配置"></a>win10+VS2017+CUDA10.0项目配置</h1><p>请先安装VS2017，一定要在安装CUDA前安装。<br>首先从这里下载CUDA10.0:<a href="https://developer.nvidia.com/cuda-10.0-download-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-10.0-download-archive</a> 。按默认选项安装。然后下载CUDNN7.4.2 for CUDA10:<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-archive</a> 。解压后复制到CUDA安装文件夹里。<br>在安CUDA装过程中，会自动检测本机是否已经安装了配套的VS版本其中之一，如果VS版本和Cuda版本不匹配的话，安装无法进行。<br>CUDA安装完成后在系统变量中应当会有下面两个变量，如果没有请自己添加上:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CUDA_PATH = C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0</span><br><span class="line">CUDA_PATH_V10_0 = C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0</span><br></pre></td></tr></table></figure>

<p>我们还要在系统变量中添加一个CUDNN变量:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDNN=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0</span><br></pre></td></tr></table></figure>

<p>我们还需要在用户变量中添加下列变量:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CUDA_SDK_PATH = C:\ProgramData\NVIDIA Corporation\CUDA Samples\v10.0</span><br><span class="line">CUDA_LIB_PATH = %CUDA_PATH%\lib\x64</span><br><span class="line">CUDA_BIN_PATH = %CUDA_PATH%\bin</span><br><span class="line">CUDA_SDK_BIN_PATH = %CUDA_SDK_PATH%\bin\win64</span><br><span class="line">CUDA_SDK_LIB_PATH = %CUDA_SDK_PATH%\common\lib\x64</span><br></pre></td></tr></table></figure>

<p>添加完成后打开cmd，使用下列命令测试CUDA是否正常:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nvcc -V //正常显示版本号则说明安装成功</span><br><span class="line">set cuda //可以查看设置的cuda环境变量</span><br><span class="line">cd C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\extras\demo_suite</span><br><span class="line">deviceQuery.exe</span><br><span class="line">bandwidthTest.exe</span><br><span class="line">// 上面两行命令都返回Rsult=PASS则说明通过，返回Rsult=Fail则需要重新安装</span><br></pre></td></tr></table></figure>

<p>打开VS2017，新建一个CUDA10.0 Runtime项目。新建项目时选择NVIDIA-&gt;CUDA 10.0-&gt;CUDA 10.0 Runtime。<br>在kernel.cu中添加下列代码：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"cuda_runtime.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"device_launch_parameters.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> deviceCount;</span><br><span class="line">	cudaGetDeviceCount(&amp;deviceCount);</span><br><span class="line">	<span class="keyword">int</span> dev;</span><br><span class="line">	<span class="keyword">for</span> (dev = <span class="number">0</span>; dev &lt; deviceCount; dev++) &#123;</span><br><span class="line">		int driver_version(0), runtime_version(0);</span><br><span class="line">		cudaDeviceProp deviceProp;</span><br><span class="line">		cudaGetDeviceProperties(&amp;deviceProp, dev);</span><br><span class="line">		<span class="keyword">if</span> (dev == <span class="number">0</span>)</span><br><span class="line">			<span class="keyword">if</span> (deviceProp.minor = <span class="number">9999</span> &amp;&amp; deviceProp.major == <span class="number">9999</span>)</span><br><span class="line">				<span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"\nDevice%d:\"%s\"\n"</span>, dev, deviceProp.name);</span><br><span class="line">		cudaDriverGetVersion(&amp;driver_version);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"CUDA驱动版本:                                   %d.%d\n"</span>, driver_version / <span class="number">1000</span>, (driver_version % <span class="number">1000</span>) / <span class="number">10</span>);</span><br><span class="line">		cudaRuntimeGetVersion(&amp;runtime_version);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"CUDA运行时版本:                                 %d.%d\n"</span>, runtime_version / <span class="number">1000</span>, (runtime_version % <span class="number">1000</span>) / <span class="number">10</span>);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"设备计算能力:                                   %d.%d\n"</span>, deviceProp.major, deviceProp.minor);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"Total amount of Global Memory:                  %u bytes\n"</span>, deviceProp.totalGlobalMem);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"Number of SMs:                                  %d\n"</span>, deviceProp.multiProcessorCount);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"Total amount of Constant Memory:                %u bytes\n"</span>, deviceProp.totalConstMem);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"Total amount of Shared Memory per block:        %u bytes\n"</span>, deviceProp.sharedMemPerBlock);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"Total number of registers available per block:  %d\n"</span>, deviceProp.regsPerBlock);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"Warp size:                                      %d\n"</span>, deviceProp.warpSize);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"Maximum number of threads per SM:               %d\n"</span>, deviceProp.maxThreadsPerMultiProcessor);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"Maximum number of threads per block:            %d\n"</span>, deviceProp.maxThreadsPerBlock);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"Maximum size of each dimension of a block:      %d x %d x %d\n"</span>, deviceProp.maxThreadsDim[<span class="number">0</span>], deviceProp.maxThreadsDim[<span class="number">1</span>], deviceProp.maxThreadsDim[<span class="number">2</span>]);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"Maximum size of each dimension of a grid:       %d x %d x %d\n"</span>, deviceProp.maxGridSize[<span class="number">0</span>], deviceProp.maxGridSize[<span class="number">1</span>], deviceProp.maxGridSize[<span class="number">2</span>]);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"Maximum memory pitch:                           %u bytes\n"</span>, deviceProp.memPitch);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"Texture alignmemt:                              %u bytes\n"</span>, deviceProp.texturePitchAlignment);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"Clock rate:                                     %.2f GHz\n"</span>, deviceProp.clockRate * <span class="number">1e-6</span>f);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"Memory Clock rate:                              %.0f MHz\n"</span>, deviceProp.memoryClockRate * <span class="number">1e-3</span>f);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"Memory Bus Width:                               %d-bit\n"</span>, deviceProp.memoryBusWidth);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用生成-&gt;重新生成(项目名)，调试-&gt;开始调试，运行结果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Device0:&quot;GeForce GTX 1070 with Max-Q Design&quot;</span><br><span class="line">CUDA驱动版本:                                   10.1</span><br><span class="line">CUDA运行时版本:                                 10.0</span><br><span class="line">设备计算能力:                                   6.0</span><br><span class="line">Total amount of Global Memory:                  0 bytes</span><br><span class="line">Number of SMs:                                  16</span><br><span class="line">Total amount of Constant Memory:                65536 bytes</span><br><span class="line">Total amount of Shared Memory per block:        49152 bytes</span><br><span class="line">Total number of registers available per block:  65536</span><br><span class="line">Warp size:                                      32</span><br><span class="line">Maximum number of threads per SM:               2048</span><br><span class="line">Maximum number of threads per block:            1024</span><br><span class="line">Maximum size of each dimension of a block:      1024 x 1024 x 64</span><br><span class="line">Maximum size of each dimension of a grid:       2147483647 x 65535 x 65535</span><br><span class="line">Maximum memory pitch:                           2147483647 bytes</span><br><span class="line">Texture alignmemt:                              32 bytes</span><br><span class="line">Clock rate:                                     1.27 GHz</span><br><span class="line">Memory Clock rate:                              4004 MHz</span><br><span class="line">Memory Bus Width:                               256-bit</span><br></pre></td></tr></table></figure>

<h1 id="CUDA常用函数介绍"><a href="#CUDA常用函数介绍" class="headerlink" title="CUDA常用函数介绍"></a>CUDA常用函数介绍</h1><h2 id="cudaMalloc-函数"><a href="#cudaMalloc-函数" class="headerlink" title="cudaMalloc()函数"></a>cudaMalloc()函数</h2><p>函数原型:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t cudaMalloc(void** devPtr, size_t size);</span><br></pre></td></tr></table></figure>

<p>这个函数和C语言中的malloc类似，但是该函数是在device上申请一定字节大小的显存，devPtr是指向所分配内存的指针。可以将cudaMalloc()分配的指针传递给在设备/主机上执行的函数，也可以在设备代码中使用cudaMalloc()分配的指针进行设备内存读写操作。注意不可以在主机代码中使用cudaMalloc()分配的指针进行主机内存读写操作（即不能进行解引用）。</p>
<h2 id="cudaFree-函数"><a href="#cudaFree-函数" class="headerlink" title="cudaFree()函数"></a>cudaFree()函数</h2><p>函数原型:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t cudaFree (void* devPtr);</span><br></pre></td></tr></table></figure>

<p>与c语言中的free()函数一样，只是此函数释放的是cudaMalloc()分配的内存。</p>
<h2 id="cudaMemcpy-函数"><a href="#cudaMemcpy-函数" class="headerlink" title="cudaMemcpy()函数"></a>cudaMemcpy()函数</h2><p>函数原型:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t cudaMemcpy (void *dst, const void *src, size_t count, cudaMemcpyKind kind);</span><br></pre></td></tr></table></figure>

<p>与c语言中的memcpy函数一样，只是此函数可以在主机内存和GPU内存之间互相拷贝数据。cudaMemcpyKind kind表示数据拷贝方向，若kind赋值为cudaMemcpyDeviceToHost表示数据从设备内存拷贝到主机内存。<br>该函数以同步方式执行，即当函数返回时，复制操作就已经完成了，并且在输出缓冲区中包含了复制进去的内容。相应的有个异步方式执行函数cudaMemcpyAsync()。</p>
<h2 id="cudaMallocManaged-函数"><a href="#cudaMallocManaged-函数" class="headerlink" title="cudaMallocManaged()函数"></a>cudaMallocManaged()函数</h2><p>函数原型:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t cudaMallocManaged(void **devPtr, size_t size, unsigned int flag=0);</span><br></pre></td></tr></table></figure>

<p>在最初的CUDA编程中，我们往往单独在host和device上进行内存分配，并且要进行将数据从host拷贝到device上，这很容易出错的。CUDA 6.0版本之后引入统一内存（Unified Memory）来避免这种麻烦，简单来说就是使用一个托管内存来共同管理host和device中的内存，并且自动在host和device中进行数据传输。CUDA中使用cudaMallocManaged()函数分配托管内存。<br><strong>注意:</strong><br>kernel核函数的执行是与host异步的，我们要在执行完kernel核函数后用cudaDeviceSynchronize()函数保证device和host同步，这样后面才可以正确访问kernel计算的结果。</p>
<h1 id="CUDA矩阵乘法实例"><a href="#CUDA矩阵乘法实例" class="headerlink" title="CUDA矩阵乘法实例"></a>CUDA矩阵乘法实例</h1><p>我们要实现两个矩阵的乘法，设输入矩阵为A和B，要得到 C=AxB 。实现思路是每个线程计算C的一个元素值Cij，对于矩阵运算，应该选用grid和block为2-D。<br><strong>计算步骤:</strong><br>分配host内存，并进行数据初始化，分配device内存，并从host将数据拷贝到device上，实际代码中使用cudaMallocManaged()函数进行内存托管；<br>调用CUDA的kernel核函数在device上完成指定的运算；<br>同步device上的运算结果到host上，使用cudaDeviceSynchronize()函数来同步；<br>释放device和host上分配的内存，这步由前面定义的cudaMallocManaged()函数自动管理。<br><strong>完整代码:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"cuda_runtime.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"device_launch_parameters.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 打印显卡各项信息</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">GetCudaImformation</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> deviceCount;</span><br><span class="line">	cudaGetDeviceCount(&amp;deviceCount);</span><br><span class="line">	<span class="keyword">int</span> dev;</span><br><span class="line">	<span class="keyword">for</span> (dev = <span class="number">0</span>; dev &lt; deviceCount; dev++) &#123;</span><br><span class="line">		int driver_version(0), runtime_version(0);</span><br><span class="line">		cudaDeviceProp deviceProp;</span><br><span class="line">		cudaGetDeviceProperties(&amp;deviceProp, dev);</span><br><span class="line">		<span class="keyword">if</span> (dev == <span class="number">0</span>)</span><br><span class="line">			<span class="keyword">if</span> (deviceProp.minor = <span class="number">9999</span> &amp;&amp; deviceProp.major == <span class="number">9999</span>)</span><br><span class="line">				<span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"使用GPU device "</span> &lt;&lt; dev &lt;&lt; <span class="string">": "</span> &lt;&lt; deviceProp.name &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		cudaDriverGetVersion(&amp;driver_version);</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"CUDA驱动版本:"</span> &lt;&lt; driver_version / <span class="number">1000</span> &lt;&lt; <span class="string">"."</span> &lt;&lt; (driver_version % <span class="number">1000</span>) / <span class="number">10</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		cudaRuntimeGetVersion(&amp;runtime_version);</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"CUDA运行时版本:"</span> &lt;&lt; runtime_version / <span class="number">1000</span> &lt;&lt; <span class="string">"."</span> &lt;&lt; (runtime_version % <span class="number">1000</span>) / <span class="number">10</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"设备计算能力:"</span> &lt;&lt; deviceProp.major &lt;&lt; <span class="string">"."</span> &lt;&lt; deviceProp.minor &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"显卡时钟频率:"</span> &lt;&lt; deviceProp.clockRate * <span class="number">1e-6</span>f &lt;&lt; <span class="string">" GHz"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"内存时钟频率:"</span> &lt;&lt; deviceProp.memoryClockRate * <span class="number">1e-3</span>f &lt;&lt; <span class="string">" MHz"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"内存总线带宽:"</span> &lt;&lt; deviceProp.memoryBusWidth &lt;&lt; <span class="string">" bit"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"总显存大小:"</span> &lt;&lt; deviceProp.totalGlobalMem / (<span class="number">1024.0</span>*<span class="number">1024.0</span>) &lt;&lt; <span class="string">" MB"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"总常量内存大小:"</span> &lt;&lt; deviceProp.totalConstMem / <span class="number">1024.0</span> &lt;&lt; <span class="string">" KB"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"SM数量:"</span> &lt;&lt; deviceProp.multiProcessorCount &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"每个SM最大线程数:"</span> &lt;&lt; deviceProp.maxThreadsPerMultiProcessor &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"每个线程块(block)共享内存大小:"</span> &lt;&lt; deviceProp.sharedMemPerBlock / <span class="number">1024.0</span> &lt;&lt; <span class="string">" KB"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"每个线程块(block)的最大线程数:"</span> &lt;&lt; deviceProp.maxThreadsPerBlock &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"每个线程块(block)的最大可用寄存器数:"</span> &lt;&lt; deviceProp.regsPerBlock &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"线程束(wrap)尺寸:"</span> &lt;&lt; deviceProp.warpSize &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"每个线程块(block)各个维度最大尺寸:"</span> &lt;&lt; deviceProp.maxThreadsDim[<span class="number">0</span>] &lt;&lt; <span class="string">" x "</span> &lt;&lt; deviceProp.maxThreadsDim[<span class="number">1</span>] &lt;&lt; <span class="string">" x "</span> &lt;&lt; deviceProp.maxThreadsDim[<span class="number">2</span>] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"每个线程格(grid)各个维度最大尺寸"</span> &lt;&lt; deviceProp.maxGridSize[<span class="number">0</span>] &lt;&lt; <span class="string">" x "</span> &lt;&lt; deviceProp.maxGridSize[<span class="number">1</span>] &lt;&lt; <span class="string">" x "</span> &lt;&lt; deviceProp.maxGridSize[<span class="number">2</span>] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"最大存储间距:"</span> &lt;&lt; deviceProp.memPitch / (<span class="number">1024.0</span>*<span class="number">1024.0</span>) &lt;&lt; <span class="string">" MB"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 矩阵类型,行优先,M(row, col) = *(M.elements + row * M.width + col)</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Matrix</span> &#123;</span></span><br><span class="line">	<span class="keyword">int</span> width;</span><br><span class="line">	<span class="keyword">int</span> height;</span><br><span class="line">	<span class="keyword">float</span> *elements;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取矩阵A的位置为(row, col)元素</span></span><br><span class="line">__<span class="function">device__ <span class="keyword">float</span> <span class="title">getElement</span><span class="params">(Matrix *A, <span class="keyword">int</span> row, <span class="keyword">int</span> col)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">return</span> A-&gt;elements[row * A-&gt;width + col];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 为矩阵A的位置为(row, col)的元素赋值</span></span><br><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">setElement</span><span class="params">(Matrix *A, <span class="keyword">int</span> row, <span class="keyword">int</span> col, <span class="keyword">float</span> value)</span> </span>&#123;</span><br><span class="line">	A-&gt;elements[row * A-&gt;width + col] = value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 矩阵相乘kernel函数,2-D,每个线程计算一个元素Cij</span></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">matMulKernel</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">float</span> Cvalue = <span class="number">0.0</span>;</span><br><span class="line">	<span class="keyword">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">	<span class="keyword">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; A-&gt;width; ++i) &#123;</span><br><span class="line">		Cvalue += getElement(A, row, i) * getElement(B, i, col);</span><br><span class="line">	&#125;</span><br><span class="line">	setElement(C, row, col, Cvalue);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	GetCudaImformation();</span><br><span class="line">	<span class="keyword">int</span> width = <span class="number">1</span> &lt;&lt; <span class="number">10</span>;</span><br><span class="line">	<span class="keyword">int</span> height = <span class="number">1</span> &lt;&lt; <span class="number">10</span>;</span><br><span class="line">	Matrix *A, *B, *C;</span><br><span class="line">	<span class="comment">// 申请托管内存</span></span><br><span class="line">	cudaMallocManaged((<span class="keyword">void</span>**)&amp;A, <span class="keyword">sizeof</span>(Matrix));</span><br><span class="line">	cudaMallocManaged((<span class="keyword">void</span>**)&amp;B, <span class="keyword">sizeof</span>(Matrix));</span><br><span class="line">	cudaMallocManaged((<span class="keyword">void</span>**)&amp;C, <span class="keyword">sizeof</span>(Matrix));</span><br><span class="line">	<span class="keyword">int</span> nBytes = width * height * <span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span><br><span class="line">	cudaMallocManaged((<span class="keyword">void</span>**)&amp;A-&gt;elements, nBytes);</span><br><span class="line">	cudaMallocManaged((<span class="keyword">void</span>**)&amp;B-&gt;elements, nBytes);</span><br><span class="line">	cudaMallocManaged((<span class="keyword">void</span>**)&amp;C-&gt;elements, nBytes);</span><br><span class="line">	<span class="comment">// 初始化A\B\C矩阵的宽度和高度</span></span><br><span class="line">	A-&gt;height = height;</span><br><span class="line">	A-&gt;width = width;</span><br><span class="line">	B-&gt;height = height;</span><br><span class="line">	B-&gt;width = width;</span><br><span class="line">	C-&gt;height = height;</span><br><span class="line">	C-&gt;width = width;</span><br><span class="line">	<span class="comment">// 初始化A矩阵所有元素为1.0,B矩阵所有元素为2.0</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; width * height; ++i) &#123;</span><br><span class="line">		A-&gt;elements[i] = <span class="number">1.0</span>;</span><br><span class="line">		B-&gt;elements[i] = <span class="number">2.0</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// 定义kernel的blocksize为(32, 32)，那么grid大小为(32, 32)</span></span><br><span class="line">	<span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line">	<span class="function">dim3 <span class="title">gridSize</span><span class="params">((width + blockSize.x - <span class="number">1</span>) / blockSize.x,</span></span></span><br><span class="line"><span class="function"><span class="params">		(height + blockSize.y - <span class="number">1</span>) / blockSize.y)</span></span>;</span><br><span class="line">	<span class="comment">// 执行kernel</span></span><br><span class="line">	matMulKernel &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt; (A, B, C);</span><br><span class="line">	<span class="comment">// 同步device数据保证结果能正确访问</span></span><br><span class="line">	cudaDeviceSynchronize();</span><br><span class="line">	<span class="comment">// 检查执行结果</span></span><br><span class="line">	<span class="keyword">float</span> maxError = <span class="number">0.0</span>;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; width * height; ++i)</span><br><span class="line">		maxError = fmax(maxError, <span class="built_in">fabs</span>(C-&gt;elements[i] - <span class="number">2</span> * width));</span><br><span class="line">	<span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">	<span class="built_in">cout</span> &lt;&lt; <span class="string">"最大误差: "</span> &lt;&lt; maxError &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
            </div>
            <hr/>

            
            <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.88rem;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff;
        background-color: #22AB38;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff;
        background-color: #019FE8;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a class="reward-link btn-floating btn-large waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close"><i class="fa fa-close"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs">
                        <li class="tab wechat-tab waves-effect waves-light"><a class="active" href="#wechat">微信</a></li>
                        <li class="tab alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                    </ul>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('#reward .reward-link').on('click', function () {
            $('#rewardModal').openModal();
        });

        $('#rewardModal .close').on('click', function () {
            $('#rewardModal').closeModal();
        });
    });
</script>
            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone" data-wechat-qrcode-helper="<p>微信里点“发现”->“扫一扫”二维码便可查看分享。</p>"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            <div class="reprint">
                <p>
                    <span class="reprint-tip">
                        <i class="fa fa-exclamation-circle"></i>&nbsp;&nbsp;Reprint please specify:
                    </span>
                    <a href="https://wyg1996.cn" class="b-link-green">鱼缸屋</a>
                    <i class="fa fa-angle-right fa-lg fa-fw text-color"></i>
                    <a href="/2019/05/15/CUDA矩阵计算原理和方法/" class="b-link-green">CUDA矩阵计算原理和方法</a>
                </p>
            </div>
        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '',
        clientSecret: '',
        repo: '',
        owner: '',
        admin: null,
        id: '2019-05-15T11-13-53',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    
        <link rel="stylesheet" href="/libs/gitment/gitment-default.css">
<link rel="stylesheet" href="/css/gitment.css">

<div class="gitment-card card" data-aos="fade-up">
    <div id="gitment-content" class="card-content"></div>
</div>

<script src="/libs/gitment/gitment.js"></script>
<script>
var gitment = new Gitment({
    id: 'Wed May 15 2019 11:13:53 GMT+0800',
    owner: '',
    repo: '',
    oauth: {
        client_id: '',
        client_secret: ''
    }
});

gitment.render('gitment-content');
</script>
    

    
        <div class="disqus-card card" data-aos="fade-up">
    <div id="disqus_thread" class="card-content">
        <noscript>Please enable JavaScript to view the
            <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
    </div>
</div>

<script type="text/javascript">
    disqus_config = function () {
        this.page.url = 'https://wyg1996.cn/2019/05/15/CUDA矩阵计算原理和方法/';
        this.page.identifier = '/2019/05/15/CUDA矩阵计算原理和方法/';
        this.page.title = 'CUDA矩阵计算原理和方法';
    };
    let disqus_shortname = '';

    (function () { // DON'T EDIT BELOW THIS LINE
        let d = document, s = d.createElement('script');
        // 如：s.src = 'https://blinkfox.disqus.com/embed.js';
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
    

    
    <div class="livere-card card" data-aos="fade-up">
    <!-- 来必力City版安装代码 -->
    <div id="lv-container" class="card-content" data-id="city" data-uid="">
        <script type="text/javascript">
            (function (d, s) {
                let j, e = d.getElementsByTagName(s)[0];
                if (typeof LivereTower === 'function') {
                    return;
                }

                j = d.createElement(s);
                j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
                j.async = true;

                e.parentNode.insertBefore(j, e);
            })(document, 'script');
        </script>
        <noscript>为正常使用来必力评论功能请激活JavaScript。</noscript>
    </div>
    <!-- City版安装代码已完成 -->
</div>
    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2019/05/15/人脸识别网络facenet原理/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/17.jpg" class="responsive-img" alt="人脸识别网络facenet原理">
                        
                        <span class="card-title">人脸识别网络facenet原理</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">人脸相关任务介绍人脸相关任务其实分为两部分:人脸检测和人脸识别。人脸检测:人脸检测就是获取图像中所有人脸的位置，并对人脸进行对齐。由于原始图像中的人脸可能存在姿态、位置上的差异，我们需要在获取人脸位置后，检测人脸中的关键点，根据这些关键点将</div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="fa fa-clock-o fa-fw icon-date"></i>2019-05-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/人脸检测与识别/" class="post-category" target="_blank">
                                    人脸检测与识别
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/人脸检测与识别/" target="_blank">
                        <span class="chip bg-color">人脸检测与识别</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2019/05/08/人脸检测网络MTCNN原理/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/7.jpg" class="responsive-img" alt="人脸检测网络MTCNN原理">
                        
                        <span class="card-title">人脸检测网络MTCNN原理</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">MTCNN介绍论文:Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks。论文地址: https://kpzhang93.gi</div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2019-05-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/人脸检测与识别/" class="post-category" target="_blank">
                                    人脸检测与识别
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/人脸检测与识别/" target="_blank">
                        <span class="chip bg-color">人脸检测与识别</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + 'From: 鱼缸屋<br />'
            + 'Author: ygwu<br />'
            + 'Link: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>

    </div>
    <div class="col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });
    });
</script>
    

</main>


<footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            &copy;<a href="mailto:ygwu@mail.ustc.edu.cn" target="_blank">鱼缸屋</a>
            

            
                &nbsp;<i class="fa fa-area-chart"></i>&nbsp;站点总字数:&nbsp;
                <span class="white-color">393.7k</span>
            

            
			
                <br>
                
                <span id="busuanzi_container_site_pv">
                    <i class="fa fa-heart-o"></i>
                    本站总访问量 <span id="busuanzi_value_site_pv" class="white-color"></span>
                </span>
                
                
                <span id="busuanzi_container_site_uv">
                    <i class="fa fa-users"></i>
                    次,&nbsp;访客数 <span id="busuanzi_value_site_uv" class="white-color"></span> 人.
                </span>
                
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/wygny" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:ygwu@mail.ustc.edu.cn" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=321699849" class="tooltipped" data-tooltip="QQ联系我: 321699849" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>



    <a href="https://www.baidu.com" class="tooltipped" target="_blank" data-tooltip="访问百度" data-position="top" data-delay="50">
        <i class="fa fa-paw"></i>
</a>



    <a href="https://www.google.com" class="tooltipped" target="_blank" data-tooltip="访问谷歌" data-position="top" data-delay="50">
        <i class="fa fa-google"></i>
</a>



    <a href="https://www.bilibili.com" class="tooltipped" target="_blank" data-tooltip="访问哔哩哔哩" data-position="top" data-delay="50">
        <i class="fa fa-bullseye"></i>
</a>



    <a href="https://blog.csdn.net/zgcr654321" class="tooltipped" target="_blank" data-tooltip="访问我的CSDN博客" data-position="top" data-delay="50">
        <i class="fa fa-copyright"></i>
</a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>




</div>
    </div>
</footer>

<div class="progress-bar"></div>


<!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input" autofocus="">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
</script>
<!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


<script src="/libs/materialize/js/materialize.min.js"></script>
<script src="/libs/masonry/masonry.pkgd.min.js"></script>
<script src="/libs/aos/aos.js"></script>
<script src="/libs/scrollprogress/scrollProgress.min.js"></script>
<script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
<script src="/js/matery.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->



    <script src="/libs/others/clicklove.js"></script>


    <script async src="/libs/others/busuanzi.pure.mini.js"></script>


</body>
</html>