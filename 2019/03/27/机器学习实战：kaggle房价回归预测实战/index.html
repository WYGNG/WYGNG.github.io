<!DOCTYPE HTML>
<html lang="zh_CN">


<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="keywords" content="机器学习实战：kaggle房价回归预测实战, 鱼缸屋">
    <meta name="description" content="项目介绍项目地址:https://www.kaggle.com/c/house-prices-advanced-regression-techniques 。该项目数据集中包含79个特征，最后预测出房价特征。我们进入上面的项目地址，点击Jo">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>机器学习实战：kaggle房价回归预测实战 | 鱼缸屋</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/css/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    <style type="text/css">
        
    </style>

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
</head>


<body>

<header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="container">
            <div class="nav-wrapper">
                <div class="brand-logo">
                    <a href="/" class="waves-effect waves-light">
                        
                        <img src="/medias/logo.png" class="logo-img hide-on-small-only">
                        
                        <span class="logo-span">鱼缸屋</span>
                    </a>
                </div>
                

<a href="#" data-activates="mobile-nav" class="button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-home"></i>
            
            <span>首页</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>标签</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-bookmark"></i>
            
            <span>分类</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-archive"></i>
            
            <span>归档</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <i class="fa fa-user-circle-o"></i>
            
            <span>关于</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/friends" class="waves-effect waves-light">
            
            <i class="fa fa-address-book"></i>
            
            <span>友情链接</span>
        </a>
    </li>
    
    <li>
        <a id="toggleSearch" class="waves-effect waves-light">
            <i id="searchIcon" class="mdi-action-search" title="Search"></i>
        </a>
    </li>

</ul>

<div class="side-nav" id="mobile-nav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">鱼缸屋</div>
        <div class="logo-desc">
            
            海纳百川，有容乃大，壁立千仞，无欲则刚
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-home"></i>
                
                首页
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                标签
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-bookmark"></i>
                
                分类
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-archive"></i>
                
                归档
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-user-circle-o"></i>
                
                关于
            </a>
        </li>
        
        <li>
            <a href="/friends" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-address-book"></i>
                
                友情链接
            </a>
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/WYGNG" class="waves-effect waves-light" target="_blank">
                <i class="fa fa-github-square fa-fw"></i>来github一起玩吧！
            </a>
        </li>
        
    </ul>

    <div class="social-link">
    <a href="https://github.com/wygny" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:ygwu@mail.ustc.edu.cn" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=321699849" class="tooltipped" data-tooltip="QQ联系我: 321699849" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>



    <a href="https://www.baidu.com" class="tooltipped" target="_blank" data-tooltip="访问百度" data-position="top" data-delay="50">
        <i class="fa fa-paw"></i>
</a>



    <a href="https://www.google.com" class="tooltipped" target="_blank" data-tooltip="访问谷歌" data-position="top" data-delay="50">
        <i class="fa fa-google"></i>
</a>



    <a href="https://www.bilibili.com" class="tooltipped" target="_blank" data-tooltip="访问哔哩哔哩" data-position="top" data-delay="50">
        <i class="fa fa-bullseye"></i>
</a>



    <a href="https://blog.csdn.net/zgcr654321" class="tooltipped" target="_blank" data-tooltip="访问我的CSDN博客" data-position="top" data-delay="50">
        <i class="fa fa-copyright"></i>
</a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>




</div>
</div>

            </div>
        </div>

        
        <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/WYGNG" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="来github一起玩吧！" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>
</header>





<div class="bg-cover post-cover" style="background-image: url('/medias/featureimages/4.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        机器学习实战：kaggle房价回归预测实战
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }
</style>
<div class="row">
    <div class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/机器学习算法实践/" target="_blank">
                                <span class="chip bg-color">机器学习算法实践</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/机器学习算法实践/" class="post-category" target="_blank">
                                机器学习算法实践
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2019-03-27
                </div>

                
                    
                    <div class="info-break-policy">
                        <i class="fa fa-file-word-o fa-fw"></i>Word Count:&nbsp;&nbsp;
                        4.2k
                    </div>
                    

                    
                    <div class="info-break-policy">
                        <i class="fa fa-clock-o fa-fw"></i>Read Times:&nbsp;&nbsp;
                        17 Min
                    </div>
                    
                
				
				
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="fa fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="项目介绍"><a href="#项目介绍" class="headerlink" title="项目介绍"></a>项目介绍</h1><p>项目地址:<a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques" target="_blank" rel="noopener">https://www.kaggle.com/c/house-prices-advanced-regression-techniques</a> 。<br>该项目数据集中包含79个特征，最后预测出房价特征。<br>我们进入上面的项目地址，点击Join competition。就可以参与项目竞赛。点击Data项，可以下载训练集train.csv、测试集test.csv、提交样本sample_submission.csv以及一个数据集描述文件data_description.txt。<br>我们自己对数据集进行预处理，并自己学习一个模型，用来预测官方测试集，最终将预测结果保存为与提交样本sample_submission.csv同样格式的.csv文件，然后点击submit Prediction提交预测结果文件即可(注意提交次数是有限制的)。</p>
<h1 id="房价回归Bagging方法预测"><a href="#房价回归Bagging方法预测" class="headerlink" title="房价回归Bagging方法预测"></a>房价回归Bagging方法预测</h1><p>这是一个比较基础的预测。首先我们要对数据集进行预处理。主要包括以下几步:</p>
<ul>
<li>为了使房价数据更加平滑和更加服从高斯分布，我们将房价用log1p函数(即log(x+1))处理，并创建一个新的dataframe对象将原始房价数据和处理后的房价数据做直方图对比，可以发现处理后的房价数据更加服从高斯分布；</li>
<li>将训练集的房价特征先单独拿出来，然后将训练集79个特征的所有行数据与测试集数据合并，一块儿处理；</li>
<li>MSSubClass特征是一个类别，但pandas会将其默认处理成数字，我们将其转化为str，然后用get_dummies方法将79个特征中凡是属于类别的数据都转化成one-hot编码格式(比如某个特征中有16个类，那么get_dummies方法会将该特征处理成16列的特征)；</li>
<li>将所有缺失值都用对应列的平均值填充；</li>
<li>所有非one_hot编码的特征列全部归一化；</li>
<li>最后将dataframe对象中所有值转换成numpy array形式。</li>
</ul>
<p>我们使用岭回归和随机森林作为bagging方法的两个个体学习器，最终预测结果取它们的算数平均值。<br>分别对岭回归模型的alpha值(正则化项前面系数)和随机森林中决策树所取特征占所有特征得百分比做网格搜索，以负均方误差的平方根作为评价指标，指标越低越好。最终得到alpha为15时最好，决策树特征=0.3(随机取30%的特征构建决策树)时最好。<br>分别学习alpha=15的岭回归模型和mat_feat=0.3的随机森林模型，分别得到它们对测试集的预测值，最后取算数平均值作为最终预测值。<br><strong>代码如下:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># index_col=0即第0号列作为index值,一般这一列都是编号,对预测没有什么作用</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">"./input/train.csv"</span>, index_col=<span class="number">0</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">"./input/test.csv"</span>, index_col=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># print(train_data.iloc[0].values)</span></span><br><span class="line"><span class="comment"># print(train_data.head())</span></span><br><span class="line">print(train_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># log1p=log(x+1),这行代码用原始数据中的房价和取对数后的房价创建了个新的DataFrame</span></span><br><span class="line"><span class="comment"># 在数据预处理时首先可以对偏度比较大的数据用log1p函数进行转化,使其更加服从高斯分布</span></span><br><span class="line"><span class="comment"># log1p可以避免出现负数结果,如果用log(x+1)后的价格做模型,预测时预测结果也不要忘记反向操作,反向函数就是expm1</span></span><br><span class="line"><span class="comment"># expm1()=exp(x)-1</span></span><br><span class="line">prices = pd.DataFrame(&#123;<span class="string">"price"</span>: train_data[<span class="string">"SalePrice"</span>], <span class="string">"log(price + 1)"</span>: np.log1p(train_data[<span class="string">"SalePrice"</span>])&#125;)</span><br><span class="line">prices.hist()</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把train_data的特征SalePrice先pop出来,然后将train_data和test_data拼合起来一起处理其他特征</span></span><br><span class="line">y_train = np.log1p(train_data.pop(<span class="string">"SalePrice"</span>))</span><br><span class="line"><span class="comment"># print(y_train.head())</span></span><br><span class="line">all_data = pd.concat((train_data, test_data), axis=<span class="number">0</span>)</span><br><span class="line">print(all_data.shape)</span><br><span class="line"><span class="comment"># MSSubClass值是一个类别,它们之间没有大小关系,但是pandas默认将其处理成数字,我们要将其转换成str</span></span><br><span class="line">all_data[<span class="string">"MSSubClass"</span>] = all_data[<span class="string">"MSSubClass"</span>].astype(str)</span><br><span class="line"><span class="comment"># print(all_df["MSSubClass"].value_counts())</span></span><br><span class="line"><span class="comment"># 使用get_dummies方法将MSSubClass转成one-hot形式编码</span></span><br><span class="line"><span class="comment"># prefix可以是字符串或字符串列表,这样就把MSSubClass这列扩展成16列的one-hot编码</span></span><br><span class="line"><span class="comment"># print(pd.get_dummies(all_df["MSSubClass"], prefix="MSSubClass").head())</span></span><br><span class="line"><span class="comment"># 我们可以将all_df中所有表示类别的特征都转成one-hot形式编码</span></span><br><span class="line">all_dummy_data = pd.get_dummies(all_data)</span><br><span class="line"><span class="comment"># [5 rows x 303 columns]</span></span><br><span class="line"><span class="comment"># print(all_dummy_df.head())</span></span><br><span class="line"><span class="comment"># isnull()判断哪一列有缺失值,.sum()统计缺失值有多少个,ascending=False表示降序排列</span></span><br><span class="line"><span class="comment"># print(all_dummy_df.isnull().sum().sort_values(ascending=False).head(10))</span></span><br><span class="line"><span class="comment"># 求所有列平均值</span></span><br><span class="line">mean_cols = all_dummy_data.mean()</span><br><span class="line"><span class="comment"># print(mean_cols.head(10))</span></span><br><span class="line"><span class="comment"># 缺失值用对应列的均值填充</span></span><br><span class="line">all_dummy_data = all_dummy_data.fillna(mean_cols)</span><br><span class="line"><span class="comment"># 检查一下,现在没有缺失值了</span></span><br><span class="line"><span class="comment"># print(all_dummy_df.isnull().sum().sum())</span></span><br><span class="line"><span class="comment"># 我们还要对数据进行归一化,注意one_hot编码形式的数据不需要归一化</span></span><br><span class="line">numeric_cols = all_data.columns[all_data.dtypes != <span class="string">"object"</span>]</span><br><span class="line"><span class="comment"># 得到需要进行归一化的特征列表</span></span><br><span class="line"><span class="comment"># print(numeric_cols)</span></span><br><span class="line"><span class="comment"># 求均值和标准差,然后进行标准化</span></span><br><span class="line">numeric_col_means = all_dummy_data.loc[:, numeric_cols].mean()</span><br><span class="line">numeric_col_std = all_dummy_data.loc[:, numeric_cols].std()</span><br><span class="line">all_dummy_data.loc[:, numeric_cols] = (all_dummy_data.loc[:, numeric_cols] - numeric_col_means) / numeric_col_std</span><br><span class="line"><span class="comment"># 数据重新分回训练集和测试集</span></span><br><span class="line">dummy_train_data = all_dummy_data.loc[train_data.index]</span><br><span class="line">dummy_test_data = all_dummy_data.loc[test_data.index]</span><br><span class="line">print(dummy_train_data.shape, dummy_test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># .values将dataframe对象转换成numpy array形式</span></span><br><span class="line">X_train = dummy_train_data.values</span><br><span class="line">X_test = dummy_test_data.values</span><br><span class="line"></span><br><span class="line"><span class="comment"># np.logspace创建等比数列,-3和2表示开始时是10的-3次方,结束时是10的2次方(包含),一共取50个数</span></span><br><span class="line">alphas = np.logspace(<span class="number">-3</span>, <span class="number">2</span>, <span class="number">50</span>)</span><br><span class="line"><span class="comment"># 以均方误差作为性能度量</span></span><br><span class="line">test_scores = []</span><br><span class="line"><span class="comment"># 网格搜索来寻找最佳alpha</span></span><br><span class="line"><span class="comment"># 使用ridge regression岭回归方法做预测</span></span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> alphas:</span><br><span class="line">   <span class="comment"># alpha值越大则岭回归的正则化项越大,必须是正浮点数,alpha对应于其他线性模型(如Logistic回归或LinearSVC)中的C^-1</span></span><br><span class="line">   clf = Ridge(alpha)</span><br><span class="line">   <span class="comment"># cross_val_score即交叉验证方法,cv=10代表10折,使用上面定义的clf模型,neg_mean_squared_error即负均方误差</span></span><br><span class="line">   test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=<span class="number">10</span>, scoring=<span class="string">"neg_mean_squared_error"</span>))</span><br><span class="line">   <span class="comment"># 记录每个alpha值使用交叉验证时得到的test_score平均值</span></span><br><span class="line">   test_scores.append(np.mean(test_score))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.plot(alphas, test_scores)</span><br><span class="line">plt.xlabel(<span class="string">"alphas"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"test scores"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定随机森林中的决策树使用的特征占比</span></span><br><span class="line">max_features = [<span class="number">.1</span>, <span class="number">.3</span>, <span class="number">.5</span>, <span class="number">.7</span>, <span class="number">.9</span>, <span class="number">.99</span>]</span><br><span class="line">test_scores = []</span><br><span class="line"><span class="comment"># 网格搜索来寻找最佳max_feat</span></span><br><span class="line"><span class="comment"># 使用随机森林模型预测</span></span><br><span class="line"><span class="keyword">for</span> max_feat <span class="keyword">in</span> max_features:</span><br><span class="line">   <span class="comment"># n_estimators为最大弱学习器的个数(决策树的个数),max_features=max_feat即决策树使用的特征占所有特征的比例</span></span><br><span class="line">   clf = RandomForestRegressor(n_estimators=<span class="number">200</span>, max_features=max_feat)</span><br><span class="line">   <span class="comment"># cross_val_score即交叉验证方法,cv=5代表5折,使用上面定义的clf模型,neg_mean_squared_error即负均方误差</span></span><br><span class="line">   test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">"neg_mean_squared_error"</span>))</span><br><span class="line">   <span class="comment"># 记录每个alpha值使用交叉验证时得到的test_score平均值</span></span><br><span class="line">   test_scores.append(np.mean(test_score))</span><br><span class="line"></span><br><span class="line">plt.plot(max_features, test_scores)</span><br><span class="line">plt.xlabel(<span class="string">"max_features"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"test_scores"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据上面网格搜索,使用岭回归时最佳alpha为15,使用随机森林时最佳max_features=0.3</span></span><br><span class="line">ridge = Ridge(alpha=<span class="number">15</span>)</span><br><span class="line">rf = RandomForestRegressor(n_estimators=<span class="number">500</span>, max_features=<span class="number">.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别学习岭回归和随机森林模型</span></span><br><span class="line">ridge.fit(X_train, y_train)</span><br><span class="line">rf.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 分别用岭回归和随机森林模型做预测</span></span><br><span class="line">y_ridge = np.expm1(ridge.predict(X_test))</span><br><span class="line">y_rf = np.expm1(rf.predict(X_test))</span><br><span class="line"><span class="comment"># bagging方法，回归问题最后取两个学习器(岭回归和随机森林)的算术平均值作为预测值</span></span><br><span class="line">y_final = (y_ridge + y_rf) / <span class="number">2</span></span><br><span class="line"><span class="comment"># submission_data即最后的预测结果</span></span><br><span class="line">submission_data = pd.DataFrame(data=&#123;<span class="string">"Id"</span>: test_data.index, <span class="string">"SalePrice"</span>: y_final&#125;)</span><br><span class="line"><span class="comment"># print(submission_data.head())</span></span><br><span class="line"></span><br><span class="line">submission_data.to_csv(<span class="string">"./input/submission_0.csv"</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h1 id="房价回归使用XGboost预测"><a href="#房价回归使用XGboost预测" class="headerlink" title="房价回归使用XGboost预测"></a>房价回归使用XGboost预测</h1><p>windows下要先在Python中安装xgboost库。在这个网址:<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#xgboost" target="_blank" rel="noopener">https://www.lfd.uci.edu/~gohlke/pythonlibs/#xgboost</a> 中下载xgboost‑0.82‑cp36‑cp36m‑win_amd64.whl文件(如果你的Python是64位3.6版本)。然后将该文件放到Python文件夹下，在该文件夹下按住shift鼠标右键打开cmd.exe安装。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install xgboost<span class="number">-0.82</span>-cp36-cp36m-win_amd64.whl</span><br></pre></td></tr></table></figure>

<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># index_col=0即第0号列作为index值,一般这一列都是编号,对预测没有什么作用</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">"./input/train.csv"</span>, index_col=<span class="number">0</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">"./input/test.csv"</span>, index_col=<span class="number">0</span>)</span><br><span class="line">print(train_data.shape, test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># log1p=log(x+1),这行代码用原始数据中的房价和取对数后的房价创建了个新的DataFrame</span></span><br><span class="line"><span class="comment"># 在数据预处理时首先可以对偏度比较大的数据用log1p函数进行转化,使其更加服从高斯分布</span></span><br><span class="line"><span class="comment"># log1p可以避免出现负数结果,如果用log(x+1)后的价格做模型,预测时预测结果也不要忘记反向操作,反向函数就是expm1=exp(x)-1</span></span><br><span class="line">prices = pd.DataFrame(&#123;<span class="string">"price"</span>: train_data[<span class="string">"SalePrice"</span>], <span class="string">"log(price + 1)"</span>: np.log1p(train_data[<span class="string">"SalePrice"</span>])&#125;)</span><br><span class="line">prices.hist()</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把train_data的特征SalePrice先pop出来,然后将train_data和test_data拼合起来一起处理其他特征</span></span><br><span class="line">y_train = np.log1p(train_data.pop(<span class="string">"SalePrice"</span>))</span><br><span class="line">all_data = pd.concat((train_data, test_data), axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># MSSubClass值是一个类别,它们之间没有大小关系,但是pandas默认将其处理成数字,我们要将其转换成str</span></span><br><span class="line">all_data[<span class="string">"MSSubClass"</span>] = all_data[<span class="string">"MSSubClass"</span>].astype(str)</span><br><span class="line"><span class="comment"># 使用get_dummies方法可将all_data中特征值是字符串的特征转成one-hot形式编码</span></span><br><span class="line"><span class="comment"># [5 rows x 303 columns]</span></span><br><span class="line">all_dummy_data = pd.get_dummies(all_data)</span><br><span class="line"><span class="comment"># 将所有缺失值用本列平均值填充</span></span><br><span class="line">mean_cols = all_dummy_data.mean()</span><br><span class="line">all_dummy_data = all_dummy_data.fillna(mean_cols)</span><br><span class="line"><span class="comment"># 对所有数据进行归一化,注意one_hot编码形式的数据不需要归一化</span></span><br><span class="line">numeric_cols = all_data.columns[all_data.dtypes != <span class="string">"object"</span>]</span><br><span class="line">numeric_col_means = all_dummy_data.loc[:, numeric_cols].mean()</span><br><span class="line">numeric_col_std = all_dummy_data.loc[:, numeric_cols].std()</span><br><span class="line">all_dummy_data.loc[:, numeric_cols] = (all_dummy_data.loc[:, numeric_cols] - numeric_col_means) / numeric_col_std</span><br><span class="line"><span class="comment"># 数据重新分回训练集和测试集</span></span><br><span class="line">dummy_train_data = all_dummy_data.loc[train_data.index]</span><br><span class="line">dummy_test_data = all_dummy_data.loc[test_data.index]</span><br><span class="line">print(dummy_train_data.shape, dummy_test_data.shape)</span><br><span class="line"><span class="comment"># .values将dataframe对象转换成numpy array形式</span></span><br><span class="line">X_train = dummy_train_data.values</span><br><span class="line">X_test = dummy_test_data.values</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 前面网格搜索已知alpha=15时岭回归模型性能最好</span></span><br><span class="line"><span class="comment"># ridge = Ridge(15)</span></span><br><span class="line"><span class="comment"># # 网格搜索对于回归问题采用bagging方法的最佳param,param即个体学习器数量,个体学习器即采用岭回归模型</span></span><br><span class="line"><span class="comment"># params = [1, 10, 15, 20, 25, 30, 40]</span></span><br><span class="line"><span class="comment"># test_scores = []</span></span><br><span class="line"><span class="comment"># for param in params:</span></span><br><span class="line"><span class="comment">#  clf = BaggingRegressor(n_estimators=param, base_estimator=ridge)</span></span><br><span class="line"><span class="comment">#  # cross_val_score即交叉验证方法,cv=10代表10折,使用上面定义的clf模型,neg_mean_squared_error即负均方误差平方根</span></span><br><span class="line"><span class="comment">#  test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=10, scoring="neg_mean_squared_error"))</span></span><br><span class="line"><span class="comment">#  test_scores.append(np.mean(test_score))</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># plt.plot(params, test_scores)</span></span><br><span class="line"><span class="comment"># plt.xlabel("params")</span></span><br><span class="line"><span class="comment"># plt.ylabel("test scores")</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"><span class="comment"># # 最佳param为15</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># params = [10, 15, 20, 25, 30, 40, 50, 60, 70, 100]</span></span><br><span class="line"><span class="comment"># test_scores = []</span></span><br><span class="line"><span class="comment"># # 网格搜索对于回归问题采用bagging方法的最佳param,param即个体学习器数量,个体学习器采用默认的决策树</span></span><br><span class="line"><span class="comment"># for param in params:</span></span><br><span class="line"><span class="comment">#  clf = BaggingRegressor(n_estimators=param)</span></span><br><span class="line"><span class="comment">#  # cross_val_score即交叉验证方法,cv=10代表10折,使用上面定义的clf模型,neg_mean_squared_error即负均方误差平方根</span></span><br><span class="line"><span class="comment">#  test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=10, scoring="neg_mean_squared_error"))</span></span><br><span class="line"><span class="comment">#  test_scores.append(np.mean(test_score))</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># plt.plot(params, test_scores)</span></span><br><span class="line"><span class="comment"># plt.xlabel("params")</span></span><br><span class="line"><span class="comment"># plt.ylabel("test scores")</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"><span class="comment"># # 最佳param为50</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># params = [10, 15, 20, 25, 30, 35, 40, 45, 50]</span></span><br><span class="line"><span class="comment"># test_scores = []</span></span><br><span class="line"><span class="comment"># for param in params:</span></span><br><span class="line"><span class="comment">#  clf = BaggingRegressor(n_estimators=param, base_estimator=ridge)</span></span><br><span class="line"><span class="comment">#  test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=10, scoring="neg_mean_squared_error"))</span></span><br><span class="line"><span class="comment">#  test_scores.append(np.mean(test_score))</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># plt.plot(params, test_scores)</span></span><br><span class="line"><span class="comment"># plt.xlabel("params")</span></span><br><span class="line"><span class="comment"># plt.ylabel("test scores")</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"><span class="comment"># # 最佳param为40</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># params = [10, 15, 20, 25, 30, 35, 40, 45, 50]</span></span><br><span class="line"><span class="comment"># test_scores = []</span></span><br><span class="line"><span class="comment"># # 网格搜索对于回归问题采用bagging方法的最佳param,param即个体学习器数量,个体学习器采用默认的决策树</span></span><br><span class="line"><span class="comment"># for param in params:</span></span><br><span class="line"><span class="comment">#  clf = BaggingRegressor(n_estimators=param)</span></span><br><span class="line"><span class="comment">#  test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=10, scoring="neg_mean_squared_error"))</span></span><br><span class="line"><span class="comment">#  test_scores.append(np.mean(test_score))</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># plt.plot(params, test_scores)</span></span><br><span class="line"><span class="comment"># plt.xlabel("params")</span></span><br><span class="line"><span class="comment"># plt.ylabel("test scores")</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"><span class="comment"># # 最佳param为50</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 上面BaggingRegressor尝试了个体学习器为决策树和岭回归模型的组合</span></span><br><span class="line"><span class="comment"># params尝试了[1, 10, 15, 20, 25, 30, 40],[10, 15, 20, 25, 30, 40, 50, 60, 70, 100]</span></span><br><span class="line"><span class="comment"># params尝试了[10, 15, 20, 25, 30, 35, 40, 45, 50]一共三种组合</span></span><br><span class="line"></span><br><span class="line">params = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">test_scores = []</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">   <span class="comment"># xgboost是梯度提升树的实现,XGBRegressor默认使用CART回归树,max_depth是回归树的最大深度,我们这里通过网格搜索找出最佳max_depth</span></span><br><span class="line">   clf = XGBRegressor(max_depth=param)</span><br><span class="line">   test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=<span class="number">10</span>, scoring=<span class="string">"neg_mean_squared_error"</span>))</span><br><span class="line">   test_scores.append(np.mean(test_score))</span><br><span class="line"></span><br><span class="line">plt.plot(params, test_scores)</span><br><span class="line">plt.xlabel(<span class="string">"params"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"test scores"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br><span class="line"><span class="comment"># 最佳param为5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后学习一个XGBRegressor模型</span></span><br><span class="line">xgboost_model = XGBRegressor(max_depth=<span class="number">5</span>)</span><br><span class="line">xgboost_model.fit(X_train, y_train)</span><br><span class="line">y_pred = np.expm1(xgboost_model.predict(X_test))</span><br><span class="line"><span class="comment"># submission_data即最后的预测结果</span></span><br><span class="line">submission_data = pd.DataFrame(data=&#123;<span class="string">"Id"</span>: test_data.index, <span class="string">"SalePrice"</span>: y_pred&#125;)</span><br><span class="line"></span><br><span class="line">submission_data.to_csv(<span class="string">"./input/submission_1.csv"</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h1 id="特征贡献度可视化-以随机森林模型为例"><a href="#特征贡献度可视化-以随机森林模型为例" class="headerlink" title="特征贡献度可视化:以随机森林模型为例"></a>特征贡献度可视化:以随机森林模型为例</h1><p>sklearn的各类模型都有一个.feature_importances_属性，用来表示各个特征对模型的贡献度。我们可以使用argsort对其进行排序，由于argsort排序是从小到大的，因此要用[::-1]进行倒序，得到从大到小的排序，返回值是特征的index下标，我们只需要查询特征名表，根据下标找到对应的特征名就可以将特征贡献度可视化了。<br>这里以随机森林模型为例，展示特征贡献度可视化。由于处理时使用了one_hot编码等方法，最终产生了303个特征，我们这里只取贡献度最大的前30个特征来可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># index_col=0即第0号列作为index值,一般这一列都是编号,对预测没有什么作用</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">"./house_prices/train.csv"</span>, index_col=<span class="number">0</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">"./house_prices/test.csv"</span>, index_col=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># print(train_data.iloc[0].values)</span></span><br><span class="line"><span class="comment"># print(train_data.head())</span></span><br><span class="line">print(train_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># log1p=log(x+1),这行代码用原始数据中的房价和取对数后的房价创建了个新的DataFrame</span></span><br><span class="line"><span class="comment"># 在数据预处理时首先可以对偏度比较大的数据用log1p函数进行转化,使其更加服从高斯分布</span></span><br><span class="line"><span class="comment"># log1p可以避免出现负数结果,如果用log(x+1)后的价格做模型,预测时预测结果也不要忘记反向操作,反向函数就是expm1</span></span><br><span class="line"><span class="comment"># expm1()=exp(x)-1</span></span><br><span class="line">prices = pd.DataFrame(&#123;<span class="string">"price"</span>: train_data[<span class="string">"SalePrice"</span>], <span class="string">"log(price + 1)"</span>: np.log1p(train_data[<span class="string">"SalePrice"</span>])&#125;)</span><br><span class="line"><span class="comment"># prices.hist()</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把train_data的特征SalePrice先pop出来,然后将train_data和test_data拼合起来一起处理其他特征</span></span><br><span class="line">y_train = np.log1p(train_data.pop(<span class="string">"SalePrice"</span>))</span><br><span class="line"><span class="comment"># print(y_train.head())</span></span><br><span class="line">all_data = pd.concat((train_data, test_data), axis=<span class="number">0</span>)</span><br><span class="line">print(all_data.shape)</span><br><span class="line"><span class="comment"># MSSubClass值是一个类别,它们之间没有大小关系,但是pandas默认将其处理成数字,我们要将其转换成str</span></span><br><span class="line">all_data[<span class="string">"MSSubClass"</span>] = all_data[<span class="string">"MSSubClass"</span>].astype(str)</span><br><span class="line"><span class="comment"># print(all_df["MSSubClass"].value_counts())</span></span><br><span class="line"><span class="comment"># 使用get_dummies方法将MSSubClass转成one-hot形式编码</span></span><br><span class="line"><span class="comment"># prefix可以是字符串或字符串列表,这样就把MSSubClass这列扩展成16列的one-hot编码</span></span><br><span class="line"><span class="comment"># print(pd.get_dummies(all_df["MSSubClass"], prefix="MSSubClass").head())</span></span><br><span class="line"><span class="comment"># 我们可以将all_df中所有表示类别的特征都转成one-hot形式编码</span></span><br><span class="line">all_dummy_data = pd.get_dummies(all_data)</span><br><span class="line"><span class="comment"># [5 rows x 303 columns]</span></span><br><span class="line"><span class="comment"># print(all_dummy_df.head())</span></span><br><span class="line"><span class="comment"># isnull()判断哪一列有缺失值,.sum()统计缺失值有多少个,ascending=False表示降序排列</span></span><br><span class="line"><span class="comment"># print(all_dummy_df.isnull().sum().sort_values(ascending=False).head(10))</span></span><br><span class="line"><span class="comment"># 求所有列平均值</span></span><br><span class="line">mean_cols = all_dummy_data.mean()</span><br><span class="line"><span class="comment"># print(mean_cols.head(10))</span></span><br><span class="line"><span class="comment"># 缺失值用对应列的均值填充</span></span><br><span class="line">all_dummy_data = all_dummy_data.fillna(mean_cols)</span><br><span class="line"><span class="comment"># 检查一下,现在没有缺失值了</span></span><br><span class="line"><span class="comment"># print(all_dummy_df.isnull().sum().sum())</span></span><br><span class="line"><span class="comment"># 我们还要对数据进行归一化,注意one_hot编码形式的数据不需要归一化</span></span><br><span class="line">numeric_cols = all_data.columns[all_data.dtypes != <span class="string">"object"</span>]</span><br><span class="line"><span class="comment"># 得到需要进行归一化的特征列表</span></span><br><span class="line"><span class="comment"># print(numeric_cols)</span></span><br><span class="line"><span class="comment"># 求均值和标准差,然后进行标准化</span></span><br><span class="line">numeric_col_means = all_dummy_data.loc[:, numeric_cols].mean()</span><br><span class="line">numeric_col_std = all_dummy_data.loc[:, numeric_cols].std()</span><br><span class="line">all_dummy_data.loc[:, numeric_cols] = (all_dummy_data.loc[:, numeric_cols] - numeric_col_means) / numeric_col_std</span><br><span class="line"><span class="comment"># 数据重新分回训练集和测试集</span></span><br><span class="line">dummy_train_data = all_dummy_data.loc[train_data.index]</span><br><span class="line">dummy_test_data = all_dummy_data.loc[test_data.index]</span><br><span class="line">print(dummy_train_data.shape, dummy_test_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># .values将dataframe对象转换成numpy array形式</span></span><br><span class="line">X_train = dummy_train_data.values</span><br><span class="line">X_test = dummy_test_data.values</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 设定随机森林中的决策树使用的特征占比</span></span><br><span class="line"><span class="comment"># max_features = [.1, .3, .5, .7, .9, .99]</span></span><br><span class="line"><span class="comment"># test_scores = []</span></span><br><span class="line"><span class="comment"># # 网格搜索来寻找最佳max_feat</span></span><br><span class="line"><span class="comment"># # 使用随机森林模型预测</span></span><br><span class="line"><span class="comment"># for max_feat in max_features:</span></span><br><span class="line"><span class="comment">#  # n_estimators为最大弱学习器的个数(决策树的个数),max_features=max_feat即决策树使用的特征占所有特征的比例</span></span><br><span class="line"><span class="comment">#  clf = RandomForestRegressor(n_estimators=200, max_features=max_feat)</span></span><br><span class="line"><span class="comment">#  # cross_val_score即交叉验证方法,cv=5代表5折,使用上面定义的clf模型,neg_mean_squared_error即负均方误差</span></span><br><span class="line"><span class="comment">#  test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=5, scoring="neg_mean_squared_error"))</span></span><br><span class="line"><span class="comment">#  # 记录每个alpha值使用交叉验证时得到的test_score平均值</span></span><br><span class="line"><span class="comment">#  test_scores.append(np.mean(test_score))</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># plt.plot(max_features, test_scores)</span></span><br><span class="line"><span class="comment"># plt.xlabel("max_features")</span></span><br><span class="line"><span class="comment"># plt.ylabel("test_scores")</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># plt.close()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据上面网格搜索,使用随机森林时最佳max_features=0.3</span></span><br><span class="line">rf = RandomForestRegressor(n_estimators=<span class="number">200</span>, max_features=<span class="number">.3</span>)</span><br><span class="line">rf.fit(X_train, y_train)</span><br><span class="line">y_rf = np.expm1(rf.predict(X_test))</span><br><span class="line">submission_data = pd.DataFrame(data=&#123;<span class="string">"Id"</span>: test_data.index, <span class="string">"SalePrice"</span>: y_rf&#125;)</span><br><span class="line"><span class="comment"># print(submission_data.head())</span></span><br><span class="line">submission_data.to_csv(<span class="string">"./house_prices/submission.csv"</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征贡献度可视化</span></span><br><span class="line">features = dummy_test_data.columns.values</span><br><span class="line"><span class="comment"># print(features)</span></span><br><span class="line">feature_importances = rf.feature_importances_</span><br><span class="line"><span class="comment"># print(feature_importances)</span></span><br><span class="line"><span class="comment"># y=x.argsort()函数是将x中的元素从小到大排列，提取其对应的index(索引)，然后输出到y</span></span><br><span class="line">indices = np.argsort(feature_importances)[::<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># 只取贡献度最高的30个特征来作图</span></span><br><span class="line">show_indices = indices[<span class="number">0</span>:<span class="number">30</span>]</span><br><span class="line"><span class="comment"># print(show_indices)</span></span><br><span class="line">num_show_features = len(show_indices)</span><br><span class="line"><span class="comment"># print(num_show_features)</span></span><br><span class="line"><span class="comment"># 将前30个特征重要度以柱状图展示</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">9</span>))</span><br><span class="line">plt.bar(range(num_show_features), feature_importances[show_indices], color=<span class="string">"g"</span>, align=<span class="string">"center"</span>)</span><br><span class="line">plt.xticks(range(num_show_features), [features[i] <span class="keyword">for</span> i <span class="keyword">in</span> show_indices], rotation=<span class="string">'45'</span>)</span><br><span class="line">plt.xlim([<span class="number">-1</span>, num_show_features])</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br><span class="line"><span class="comment"># 输出各个特征的贡献度</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> show_indices:</span><br><span class="line">   print(<span class="string">"特征&#123;&#125;贡献度为:&#123;:.3f&#125;"</span>.format(features[i], feature_importances[i]))</span><br></pre></td></tr></table></figure>
            </div>
            <hr/>

            
            <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.88rem;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff;
        background-color: #22AB38;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff;
        background-color: #019FE8;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a class="reward-link btn-floating btn-large waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close"><i class="fa fa-close"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs">
                        <li class="tab wechat-tab waves-effect waves-light"><a class="active" href="#wechat">微信</a></li>
                        <li class="tab alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                    </ul>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('#reward .reward-link').on('click', function () {
            $('#rewardModal').openModal();
        });

        $('#rewardModal .close').on('click', function () {
            $('#rewardModal').closeModal();
        });
    });
</script>
            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone" data-wechat-qrcode-helper="<p>微信里点“发现”->“扫一扫”二维码便可查看分享。</p>"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            <div class="reprint">
                <p>
                    <span class="reprint-tip">
                        <i class="fa fa-exclamation-circle"></i>&nbsp;&nbsp;Reprint please specify:
                    </span>
                    <a href="https://wyg1996.cn" class="b-link-green">鱼缸屋</a>
                    <i class="fa fa-angle-right fa-lg fa-fw text-color"></i>
                    <a href="/2019/03/27/机器学习实战：kaggle房价回归预测实战/" class="b-link-green">机器学习实战：kaggle房价回归预测实战</a>
                </p>
            </div>
        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '',
        clientSecret: '',
        repo: '',
        owner: '',
        admin: null,
        id: '2019-03-27T18-06-32',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    
        <link rel="stylesheet" href="/libs/gitment/gitment-default.css">
<link rel="stylesheet" href="/css/gitment.css">

<div class="gitment-card card" data-aos="fade-up">
    <div id="gitment-content" class="card-content"></div>
</div>

<script src="/libs/gitment/gitment.js"></script>
<script>
var gitment = new Gitment({
    id: 'Wed Mar 27 2019 18:06:32 GMT+0800',
    owner: '',
    repo: '',
    oauth: {
        client_id: '',
        client_secret: ''
    }
});

gitment.render('gitment-content');
</script>
    

    
        <div class="disqus-card card" data-aos="fade-up">
    <div id="disqus_thread" class="card-content">
        <noscript>Please enable JavaScript to view the
            <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
    </div>
</div>

<script type="text/javascript">
    disqus_config = function () {
        this.page.url = 'https://wyg1996.cn/2019/03/27/机器学习实战：kaggle房价回归预测实战/';
        this.page.identifier = '/2019/03/27/机器学习实战：kaggle房价回归预测实战/';
        this.page.title = '机器学习实战：kaggle房价回归预测实战';
    };
    let disqus_shortname = '';

    (function () { // DON'T EDIT BELOW THIS LINE
        let d = document, s = d.createElement('script');
        // 如：s.src = 'https://blinkfox.disqus.com/embed.js';
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
    

    
    <div class="livere-card card" data-aos="fade-up">
    <!-- 来必力City版安装代码 -->
    <div id="lv-container" class="card-content" data-id="city" data-uid="">
        <script type="text/javascript">
            (function (d, s) {
                let j, e = d.getElementsByTagName(s)[0];
                if (typeof LivereTower === 'function') {
                    return;
                }

                j = d.createElement(s);
                j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
                j.async = true;

                e.parentNode.insertBefore(j, e);
            })(document, 'script');
        </script>
        <noscript>为正常使用来必力评论功能请激活JavaScript。</noscript>
    </div>
    <!-- City版安装代码已完成 -->
</div>
    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2019/03/30/GAN原理、Tensorflow搭建GAN神经网络/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/14.jpg" class="responsive-img" alt="GAN原理、Tensorflow搭建GAN神经网络">
                        
                        <span class="card-title">GAN原理、Tensorflow搭建GAN神经网络</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">GAN原理概述论文:GenerativeAdversarialNets论文地址:https://arxiv.org/pdf/1406.2661.pdf 。
GAN模型中包括一个生成模型G和一个判别模型D。生成模型G接收一个均匀分布中取得的随</div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="fa fa-clock-o fa-fw icon-date"></i>2019-03-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/深度学习原理推导/" class="post-category" target="_blank">
                                    深度学习原理推导
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/深度学习原理推导/" target="_blank">
                        <span class="chip bg-color">深度学习原理推导</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2019/03/25/机器学习实战：交易数据异常检测/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/2.jpg" class="responsive-img" alt="机器学习实战:交易数据异常检测">
                        
                        <span class="card-title">机器学习实战:交易数据异常检测</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">数据集介绍项目来源:python数据分析与机器学习实战-唐宇迪 https://study.163.com/course/introduction.htm?courseId=1003590004&amp;share=1&amp;shareI</div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2019-03-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/机器学习算法实践/" class="post-category" target="_blank">
                                    机器学习算法实践
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/机器学习算法实践/" target="_blank">
                        <span class="chip bg-color">机器学习算法实践</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + 'From: 鱼缸屋<br />'
            + 'Author: ygwu<br />'
            + 'Link: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>

    </div>
    <div class="col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });
    });
</script>
    

</main>


<footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            &copy;<a href="mailto:ygwu@mail.ustc.edu.cn" target="_blank">鱼缸屋</a>
            

            
                &nbsp;<i class="fa fa-area-chart"></i>&nbsp;站点总字数:&nbsp;
                <span class="white-color">393.7k</span>
            

            
			
                <br>
                
                <span id="busuanzi_container_site_pv">
                    <i class="fa fa-heart-o"></i>
                    本站总访问量 <span id="busuanzi_value_site_pv" class="white-color"></span>
                </span>
                
                
                <span id="busuanzi_container_site_uv">
                    <i class="fa fa-users"></i>
                    次,&nbsp;访客数 <span id="busuanzi_value_site_uv" class="white-color"></span> 人.
                </span>
                
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/wygny" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:ygwu@mail.ustc.edu.cn" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=321699849" class="tooltipped" data-tooltip="QQ联系我: 321699849" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>



    <a href="https://www.baidu.com" class="tooltipped" target="_blank" data-tooltip="访问百度" data-position="top" data-delay="50">
        <i class="fa fa-paw"></i>
</a>



    <a href="https://www.google.com" class="tooltipped" target="_blank" data-tooltip="访问谷歌" data-position="top" data-delay="50">
        <i class="fa fa-google"></i>
</a>



    <a href="https://www.bilibili.com" class="tooltipped" target="_blank" data-tooltip="访问哔哩哔哩" data-position="top" data-delay="50">
        <i class="fa fa-bullseye"></i>
</a>



    <a href="https://blog.csdn.net/zgcr654321" class="tooltipped" target="_blank" data-tooltip="访问我的CSDN博客" data-position="top" data-delay="50">
        <i class="fa fa-copyright"></i>
</a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>




</div>
    </div>
</footer>

<div class="progress-bar"></div>


<!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input" autofocus="">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
</script>
<!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


<script src="/libs/materialize/js/materialize.min.js"></script>
<script src="/libs/masonry/masonry.pkgd.min.js"></script>
<script src="/libs/aos/aos.js"></script>
<script src="/libs/scrollprogress/scrollProgress.min.js"></script>
<script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
<script src="/js/matery.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->



    <script src="/libs/others/clicklove.js"></script>


    <script async src="/libs/others/busuanzi.pure.mini.js"></script>


</body>
</html>