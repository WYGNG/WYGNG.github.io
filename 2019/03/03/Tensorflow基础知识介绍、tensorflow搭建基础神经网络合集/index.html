<!DOCTYPE HTML>
<html lang="zh_CN">


<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="keywords" content="Tensorflow基础知识介绍、Tensorflow搭建基础神经网络合集, 修改">
    <meta name="description" content="Tensorflow介绍Tensorflow由Google开发，是GitHub上最受欢迎的机器学习/深度学习库之一。TensorFlow的核心概念是节点、张量和计算图。节点一般表示施加的某个数学操作，也可以表示数据输入的起点/输出的终点，或">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>Tensorflow基础知识介绍、Tensorflow搭建基础神经网络合集 | 修改</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/css/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    <style type="text/css">
        
    </style>

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
</head>


<body>

<header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="container">
            <div class="nav-wrapper">
                <div class="brand-logo">
                    <a href="/" class="waves-effect waves-light">
                        
                        <img src="/medias/logo.png" class="logo-img hide-on-small-only">
                        
                        <span class="logo-span">修改</span>
                    </a>
                </div>
                

<a href="#" data-activates="mobile-nav" class="button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-home"></i>
            
            <span>首页</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>标签</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-bookmark"></i>
            
            <span>分类</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-archive"></i>
            
            <span>归档</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <i class="fa fa-user-circle-o"></i>
            
            <span>关于</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/friends" class="waves-effect waves-light">
            
            <i class="fa fa-address-book"></i>
            
            <span>友情链接</span>
        </a>
    </li>
    
    <li>
        <a id="toggleSearch" class="waves-effect waves-light">
            <i id="searchIcon" class="mdi-action-search" title="Search"></i>
        </a>
    </li>

</ul>

<div class="side-nav" id="mobile-nav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">修改</div>
        <div class="logo-desc">
            
            修改
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-home"></i>
                
                首页
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                标签
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-bookmark"></i>
                
                分类
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-archive"></i>
                
                归档
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-user-circle-o"></i>
                
                关于
            </a>
        </li>
        
        <li>
            <a href="/friends" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-address-book"></i>
                
                友情链接
            </a>
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/WYGNG" class="waves-effect waves-light" target="_blank">
                <i class="fa fa-github-square fa-fw"></i>来github一起玩吧！
            </a>
        </li>
        
    </ul>

    <div class="social-link">
    <a href="https://github.com/wygny" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:ygwu@mail.ustc.edu.cn" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=321699849" class="tooltipped" data-tooltip="QQ联系我: 321699849" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>



    <a href="https://www.baidu.com" class="tooltipped" target="_blank" data-tooltip="访问百度" data-position="top" data-delay="50">
        <i class="fa fa-paw"></i>
</a>



    <a href="https://www.google.com" class="tooltipped" target="_blank" data-tooltip="访问谷歌" data-position="top" data-delay="50">
        <i class="fa fa-google"></i>
</a>



    <a href="https://www.bilibili.com" class="tooltipped" target="_blank" data-tooltip="访问哔哩哔哩" data-position="top" data-delay="50">
        <i class="fa fa-bullseye"></i>
</a>



    <a href="https://blog.csdn.net/zgcr654321" class="tooltipped" target="_blank" data-tooltip="访问我的CSDN博客" data-position="top" data-delay="50">
        <i class="fa fa-copyright"></i>
</a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>




</div>
</div>

            </div>
        </div>

        
        <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/WYGNG" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="来github一起玩吧！" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>
</header>





<div class="bg-cover post-cover" style="background-image: url('/medias/featureimages/11.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        Tensorflow基础知识介绍、Tensorflow搭建基础神经网络合集
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }
</style>
<div class="row">
    <div class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Tensorflow/" target="_blank">
                                <span class="chip bg-color">Tensorflow</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Tensorflow/" class="post-category" target="_blank">
                                Tensorflow
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2019-03-03
                </div>

                
                    
                    <div class="info-break-policy">
                        <i class="fa fa-file-word-o fa-fw"></i>Word Count:&nbsp;&nbsp;
                        7.6k
                    </div>
                    

                    
                    <div class="info-break-policy">
                        <i class="fa fa-clock-o fa-fw"></i>Read Times:&nbsp;&nbsp;
                        35 Min
                    </div>
                    
                
				
				
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="fa fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="Tensorflow介绍"><a href="#Tensorflow介绍" class="headerlink" title="Tensorflow介绍"></a>Tensorflow介绍</h1><p>Tensorflow由Google开发，是GitHub上最受欢迎的机器学习/深度学习库之一。TensorFlow的核心概念是节点、张量和计算图。<br>节点一般表示施加的某个数学操作，也可以表示数据输入的起点/输出的终点，或者是读取/写入持久变量的终点。线表示之间的输入/输出关系。张量可以理解为是不同维度的矩阵，张量沿着线的方向在计算图中流动，这就是取名为Tensorflow的原因。<br>需要注意的是，在TensorFlow中，我们必须要先完整地构建一个计算图，然后按照计算图启动一个会话，然后才能在会话中完成变量的初始化和计算，最终得到结果。</p>
<h1 id="Tensorflow张量、placeholder占位符、Session会话"><a href="#Tensorflow张量、placeholder占位符、Session会话" class="headerlink" title="Tensorflow张量、placeholder占位符、Session会话"></a>Tensorflow张量、placeholder占位符、Session会话</h1><p>在TensorFlow中，可以将张量理解为数组。如果是0阶张量，那么这个张量是一个标量，也就是一个数字，如果是一阶张量可以理解为向量或者是一维数组，n阶张量可以理解为n维的数组。张量中包含了三个重要的属性，名字、维度、类型。<br>TensorFlow张量的实现并没有直接采用数组的形式，张量它只是对运算结果的引用，如果我们直接print某个张量，我们只能打印出一个指向该张量内容的指针，而不能打印出张量的内容。<br>如果我们想创建两个张量，但这两个张量的值需要从外部输入，这时我们可以用tf.placeholder创建placeholder占位符张量，占位符张量的值可以在Session会话时输入。<br>Session会话会话用来执行定义好的运算图，我们在会话中初始化所有变量，输入样本数据后，就可以开始计算了。当计算完成之后，需要通过关闭会话来帮助系统回收资源，否则可能导致资源泄露的问题。<br>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 张量,采用高斯分布初始化和常量初始化</span></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">10</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义占位符</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="number">1</span>, <span class="number">2</span>), name=<span class="string">"input"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话开始计算,with as这种形式创建的会话执行完运算后会自动关闭会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   sess.run(tf.global_variables_initializer())</span><br><span class="line">   print(sess.run(y, feed_dict=&#123;x: [[<span class="number">0.7</span>, <span class="number">0.9</span>]]&#125;))</span><br></pre></td></tr></table></figure>

<p>我们还可以定义多个计算图，在创建会话时指定某个计算图进行计算。<br>我们还可以通过tf.ConfigProto()函数，配置会话并行的线程数、GPU的分配策略、运算超时等参数。<br>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建g1计算图</span></span><br><span class="line">g1 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g1.as_default():</span><br><span class="line">   v = tf.get_variable(<span class="string">'v'</span>, [<span class="number">1</span>], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建g2计算图</span></span><br><span class="line">g2 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g2.as_default():</span><br><span class="line">   v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">2</span>], initializer=tf.ones_initializer())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个会话计算g1计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g1) <span class="keyword">as</span> sess:</span><br><span class="line">   tf.global_variables_initializer().run()</span><br><span class="line">   <span class="comment"># 创建一个命名空间,将参数reuse设置为True.这样tf.get_variable函数将直接获取已经声明的变量</span></span><br><span class="line">   <span class="keyword">with</span> tf.variable_scope(<span class="string">""</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">      print(sess.run(tf.get_variable(<span class="string">"v"</span>)))</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([<span class="number">2.0</span>, <span class="number">3.0</span>], name=<span class="string">'b'</span>)</span><br><span class="line">c = a + b</span><br><span class="line">print(c)</span><br><span class="line"><span class="comment"># tf.InteractiveSession()是一种交互式的session方式，它让自己成为默认的session</span></span><br><span class="line"><span class="comment"># 用户在不需要指明用哪个session运行的情况下,就可以运行起来,run()和eval()函数都可以不指明是哪个session</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">print(c.eval())</span><br><span class="line">sess.close()</span><br><span class="line"><span class="comment"># 创建一个默认会话,会运行g1计算图之后到这个会话之前中间定义的计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   print(<span class="string">"a+b:"</span>, sess.run(c))</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.ConfigProto()函数可配置会话并行的线程数、GPU的分配策略、运算超时等参数</span></span><br><span class="line"><span class="comment"># allow_soft_placement=True:如果你指定的设备不存在，允许TF自动分配设备;log_device_placement=True为是否打印设备分配日志</span></span><br><span class="line">config = tf.ConfigProto(allow_soft_placement=<span class="literal">True</span>, log_device_placement=<span class="literal">True</span>)</span><br><span class="line">sess1 = tf.InteractiveSession(config=config)</span><br><span class="line">sess2 = tf.Session(config=config)</span><br><span class="line">print(<span class="string">"configproto:"</span>)</span><br><span class="line">print(sess1.run(c))</span><br><span class="line">print(sess2.run(c))</span><br></pre></td></tr></table></figure>

<h1 id="使用tf-data-Dataset加载数据，加快训练过程"><a href="#使用tf-data-Dataset加载数据，加快训练过程" class="headerlink" title="使用tf.data.Dataset加载数据，加快训练过程"></a>使用tf.data.Dataset加载数据，加快训练过程</h1><p>使用feed_dict加载数据这个过程是CPU完成的，此时GPU空闲未工作，如果我们加载数据需要的时间比较多，就会大大拉长我们的模型训练时间。<br>我们可以使用tf.data API来加载数据。在tf.data工作流中，能够以异步方式预读取下个批次的数据，这样就可以缩短GPU等待加载数据的时间(如果训练一个batch的时间大于读取下个批次数据的时间，那么GPU就可以连续地进行计算)。<br>下面是分别使用numpy数组、tensorflow张量、tensorflow占位符、tensorflow生成器创建dataset对象的实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面的设置可以使这个警告不出现:Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2</span></span><br><span class="line">os.environ[<span class="string">"TF_CPP_MIN_LOG_LEVEL"</span>] = <span class="string">"2"</span></span><br><span class="line"><span class="comment"># 指定使用的GPU的编号，从0开始</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"0"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建numpy数组形式的数据集</span></span><br><span class="line">x = np.random.sample((<span class="number">100</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 创建一个dataset对象,传入数据集x</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices(x)</span><br><span class="line"><span class="comment"># 用dataset对象创建一个One Shot迭代器,One Shot迭代器在会话中不需要初始化</span></span><br><span class="line">iter_1 = dataset.make_one_shot_iterator()</span><br><span class="line"><span class="comment"># 取迭代器下一个数据的操作</span></span><br><span class="line">el_1 = iter_1.get_next()</span><br><span class="line"><span class="comment"># 创建会话,运行</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   print(sess.run(el_1))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建有特征和标签的数据集</span></span><br><span class="line">features, labels = (np.random.sample((<span class="number">100</span>, <span class="number">2</span>)), np.random.sample((<span class="number">100</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="comment"># 创建dataset对象,再创建One Shot迭代器和取迭代器下一个数据的操作</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices((features, labels))</span><br><span class="line">iter_2 = dataset.make_one_shot_iterator()</span><br><span class="line">el_2 = iter_2.get_next()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   <span class="comment"># 连续取5次迭代器下一个数据</span></span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">      print(sess.run(el_2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接使用tensorflow张量创建dataset对象</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([<span class="number">100</span>, <span class="number">2</span>]))</span><br><span class="line"><span class="comment"># 这里要创建可初始化迭代器,定义取迭代器下一个元素对象操作,可初始化迭代器在会话中必须先初始化</span></span><br><span class="line">iter_3 = dataset.make_initializable_iterator()</span><br><span class="line">el_3 = iter_3.get_next()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   sess.run(iter_3.initializer)</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">      print(sess.run(el_3))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用tensorflow占位符变量创建dataset对象</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">2</span>])</span><br><span class="line">y = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices((x, y))</span><br><span class="line">features, labels = (np.random.sample((<span class="number">100</span>, <span class="number">2</span>)), np.random.sample((<span class="number">100</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="comment"># 这里要创建可初始化迭代器,定义取迭代器下一个元素对象操作,可初始化迭代器在会话中必须先初始化和定义feed_dict</span></span><br><span class="line">iter_4 = dataset.make_initializable_iterator()</span><br><span class="line">el_4 = iter_4.get_next()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   sess.run(iter_4.initializer, feed_dict=&#123;x: features, y: labels&#125;)</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">      print(sess.run(el_4))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从生成器创建dataset对象</span></span><br><span class="line">sequence = np.array([[[<span class="number">1</span>]], [[<span class="number">2</span>], [<span class="number">3</span>]], [[<span class="number">4</span>], [<span class="number">5</span>], [<span class="number">6</span>]]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个生成器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">   <span class="keyword">for</span> el <span class="keyword">in</span> sequence:</span><br><span class="line">      <span class="keyword">yield</span> el</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用生成器创建dataset对象</span></span><br><span class="line">dataset = tf.data.Dataset().batch(<span class="number">1</span>).from_generator(generator, output_types=tf.int64,</span><br><span class="line">                                                    output_shapes=(tf.TensorShape([<span class="literal">None</span>, <span class="number">1</span>])))</span><br><span class="line"><span class="comment"># 创建可初始化迭代器</span></span><br><span class="line">iter_5 = dataset.make_initializable_iterator()</span><br><span class="line">el_5 = iter_5.get_next()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   sess.run(iter_5.initializer)</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">      print(sess.run(el_5))</span><br></pre></td></tr></table></figure>

<p>如果我们想在神经网络训练和测试时使用dataset对象取训练样本和测试样本，可看下面这个用例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面的设置可以使这个警告不出现:Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2</span></span><br><span class="line">os.environ[<span class="string">"TF_CPP_MIN_LOG_LEVEL"</span>] = <span class="string">"2"</span></span><br><span class="line"><span class="comment"># 指定使用的GPU的编号，从0开始</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"0"</span></span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">4</span></span><br><span class="line">train_data = (np.random.sample((<span class="number">100</span>, <span class="number">2</span>)), np.random.sample((<span class="number">100</span>, <span class="number">1</span>)))</span><br><span class="line">test_data = (np.random.sample((<span class="number">10</span>, <span class="number">2</span>)), np.random.sample((<span class="number">10</span>, <span class="number">1</span>)))</span><br><span class="line">x, y = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">2</span>]), tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(BATCH_SIZE)</span><br><span class="line"><span class="comment"># shuffle()来打乱数据集样本顺序,参数buffer_size即下一个元素将从该buffer_size大小的缓存中选取</span></span><br><span class="line">dataset = dataset.shuffle(buffer_size=<span class="number">100</span>)</span><br><span class="line">iter_1 = dataset.make_initializable_iterator()</span><br><span class="line">features, labels = iter_1.get_next()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   sess.run(iter_1.initializer, feed_dict=&#123;x: train_data[<span class="number">0</span>], y: train_data[<span class="number">1</span>]&#125;)</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">      print(sess.run([features, labels]))</span><br><span class="line">   sess.run(iter_1.initializer, feed_dict=&#123;x: test_data[<span class="number">0</span>], y: test_data[<span class="number">1</span>]&#125;)</span><br><span class="line">   print(sess.run([features, labels]))</span><br></pre></td></tr></table></figure>

<h1 id="Tensorflow保存和恢复模型"><a href="#Tensorflow保存和恢复模型" class="headerlink" title="Tensorflow保存和恢复模型"></a>Tensorflow保存和恢复模型</h1><p>tensorflow中保存和恢复模型需要创建一个tf.train.Saver对象，使用.save()方法即可保存，使用.restore()方法可恢复模型。注意保存时的目录必须先手动建好，否则无法保存。<br>用例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line">model_path = <span class="string">"./tmp/model.ckpt"</span></span><br><span class="line">n_hidden_1 = <span class="number">256</span></span><br><span class="line">n_hidden_2 = <span class="number">256</span></span><br><span class="line">n_input = <span class="number">784</span></span><br><span class="line">n_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义x和y占位符</span></span><br><span class="line">X, Y = tf.placeholder(tf.float32, [<span class="literal">None</span>, n_input]), tf.placeholder(tf.float32, [<span class="literal">None</span>, n_classes])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multilayer_perceptron</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">   layer_1 = tf.nn.relu(tf.add(tf.matmul(x, w[<span class="string">"h1"</span>]), b[<span class="string">"b1"</span>]))</span><br><span class="line">   layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, w[<span class="string">"h2"</span>]), b[<span class="string">"b2"</span>]))</span><br><span class="line">   out_layer = tf.matmul(layer_2, w[<span class="string">"out"</span>]) + b[<span class="string">"out"</span>]</span><br><span class="line">   <span class="keyword">return</span> out_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义weights和biases</span></span><br><span class="line">weights = &#123;</span><br><span class="line">   <span class="string">"h1"</span>: tf.Variable(tf.random_normal([n_input, n_hidden_1])),</span><br><span class="line">   <span class="string">"h2"</span>: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),</span><br><span class="line">   <span class="string">"out"</span>: tf.Variable(tf.random_normal([n_hidden_2, n_classes]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">biases = &#123;</span><br><span class="line">   <span class="string">"b1"</span>: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">   <span class="string">"b2"</span>: tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">   <span class="string">"out"</span>: tf.Variable(tf.random_normal([n_classes]))</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 预测值</span></span><br><span class="line">pred = multilayer_perceptron(X, weights, biases)</span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">"./tmp/"</span>):</span><br><span class="line">   os.mkdir(<span class="string">"./tmp/"</span>)</span><br><span class="line"><span class="comment"># 定义saver对象，用来保存/恢复模型</span></span><br><span class="line">saver = tf.train.Saver(max_to_keep=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   sess.run(tf.global_variables_initializer())</span><br><span class="line">   <span class="comment"># 训练模型</span></span><br><span class="line">   <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">      avg_cost = <span class="number">0.</span></span><br><span class="line">      total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">         batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">         _, c = sess.run([optimizer, cost], feed_dict=&#123;X: batch_x, Y: batch_y&#125;)</span><br><span class="line">         avg_cost += c / total_batch</span><br><span class="line">      <span class="comment"># 每一轮epoch后打印一次cost值</span></span><br><span class="line">      <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</span><br><span class="line">         print(<span class="string">"Epoch=&#123;:04d&#125;,cost=&#123;:.9f&#125;"</span>.format((epoch + <span class="number">1</span>), avg_cost))</span><br><span class="line">         <span class="comment"># 保存模型</span></span><br><span class="line">         save_path = saver.save(sess, model_path, global_step=epoch)</span><br><span class="line">         print(<span class="string">"模型保存到文件夹:&#123;&#125;"</span>.format(save_path))</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 测试模型准确率</span></span><br><span class="line">   correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(Y, <span class="number">1</span>))</span><br><span class="line">   accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">   print(<span class="string">"Test accuracy=&#123;&#125;"</span>.format(accuracy.eval(&#123;X: mnist.test.images, Y: mnist.test.labels&#125;)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   sess.run(tf.global_variables_initializer())</span><br><span class="line">   <span class="comment"># 恢复模型</span></span><br><span class="line">   <span class="keyword">if</span> os.path.exists(<span class="string">"./tmp/checkpoint"</span>):</span><br><span class="line">      <span class="comment"># 判断最新的保存模型检查点是否存在，如果存在则从最近的检查点恢复模型</span></span><br><span class="line">      saver.restore(sess, tf.train.latest_checkpoint(<span class="string">"./tmp/"</span>))</span><br><span class="line">   saver.restore(sess, model_path)</span><br><span class="line">   <span class="comment"># 继续训练模型</span></span><br><span class="line">   <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">      avg_cost = <span class="number">0.</span></span><br><span class="line">      total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">         batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">         _, c = sess.run([optimizer, cost], feed_dict=&#123;X: batch_x, Y: batch_y&#125;)</span><br><span class="line">         avg_cost += c / total_batch</span><br><span class="line">      <span class="comment"># 每过一轮epoch打印cost值</span></span><br><span class="line">      <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</span><br><span class="line">         print(<span class="string">"Epoch=&#123;:04d&#125;,cost=&#123;:.9f&#125;"</span>.format((epoch + <span class="number">1</span>), avg_cost))</span><br><span class="line">         <span class="comment"># 保存模型</span></span><br><span class="line">         save_path = saver.save(sess, model_path, global_step=epoch)</span><br><span class="line">         print(<span class="string">"模型保存到文件夹:&#123;&#125;"</span>.format(save_path))</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 测试模型准确率</span></span><br><span class="line">   correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(Y, <span class="number">1</span>))</span><br><span class="line">   accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">   print(<span class="string">"Test accuracy=&#123;&#125;"</span>.format(accuracy.eval(&#123;X: mnist.test.images, Y: mnist.test.labels&#125;)))</span><br></pre></td></tr></table></figure>

<h1 id="Tensorflow-GPU使用设置-log日志显示级别设置"><a href="#Tensorflow-GPU使用设置-log日志显示级别设置" class="headerlink" title="Tensorflow GPU使用设置/log日志显示级别设置"></a>Tensorflow GPU使用设置/log日志显示级别设置</h1><p>设置使用哪块GPU：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只使用第0块GPU，编号是从0开始的，或者同时使用多块GPU，如"0"改为"0,1,2"</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"0"</span></span><br></pre></td></tr></table></figure>

<p>设置log日志级别：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"TF_CPP_MIN_LOG_LEVEL"</span>]=<span class="string">"1"</span> <span class="comment"># 这是默认的显示等级，显示所有信息</span></span><br><span class="line">os.environ[<span class="string">"TF_CPP_MIN_LOG_LEVEL"</span>]=<span class="string">"2"</span> <span class="comment"># 只显示 warning 和 Error </span></span><br><span class="line">os.environ[<span class="string">"TF_CPP_MIN_LOG_LEVEL"</span>]=<span class="string">"3"</span> <span class="comment"># 只显示 Error</span></span><br></pre></td></tr></table></figure>

<h1 id="Tensorboard工具的使用"><a href="#Tensorboard工具的使用" class="headerlink" title="Tensorboard工具的使用"></a>Tensorboard工具的使用</h1><p>Tensorboard可以记录与展示以下数据形式:</p>
<ul>
<li>标量Scalars；</li>
<li>图片Images；</li>
<li>音频Audio；</li>
<li>计算图Graph； </li>
<li>数据分布Distribution；</li>
<li>嵌入向量Embeddings。</li>
</ul>
<p>使用tf.summary.scalar记录标量；使用tf.summary.histogram记录数据的直方图；使用tf.summary.distribution记录数据的分布图；使用tf.summary.image记录图像(比如每层卷积出来的特征图显示成图像)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"TF_CPP_MIN_LOG_LEVEL"</span>] = <span class="string">"2"</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"0"</span></span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">25</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line">logs_path = <span class="string">"./tmp/"</span></span><br><span class="line"></span><br><span class="line">n_hidden_1 = <span class="number">256</span></span><br><span class="line">n_hidden_2 = <span class="number">256</span></span><br><span class="line">n_input = <span class="number">784</span></span><br><span class="line">n_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>], name=<span class="string">"InputData"</span>)</span><br><span class="line">Y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>], name=<span class="string">"LabelData"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multilayer_perceptron</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">   layer_1 = tf.nn.relu(tf.add(tf.matmul(x, w[<span class="string">"w1"</span>]), b[<span class="string">"b1"</span>]))</span><br><span class="line">   tf.summary.histogram(<span class="string">"layer_1"</span>, layer_1)</span><br><span class="line">   layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, w[<span class="string">"w2"</span>]), b[<span class="string">"b2"</span>]))</span><br><span class="line">   tf.summary.histogram(<span class="string">"layer_2"</span>, layer_2)</span><br><span class="line">   out_layer = tf.add(tf.matmul(layer_2, w[<span class="string">"w3"</span>]), b[<span class="string">"b3"</span>])</span><br><span class="line">   <span class="keyword">return</span> out_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">weights = &#123;</span><br><span class="line">   <span class="string">"w1"</span>: tf.Variable(tf.random_normal([n_input, n_hidden_1]), name=<span class="string">"W1"</span>),</span><br><span class="line">   <span class="string">"w2"</span>: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]), name=<span class="string">"W2"</span>),</span><br><span class="line">   <span class="string">"w3"</span>: tf.Variable(tf.random_normal([n_hidden_2, n_classes]), name=<span class="string">"W3"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">biases = &#123;</span><br><span class="line">   <span class="string">"b1"</span>: tf.Variable(tf.random_normal([n_hidden_1]), name=<span class="string">"b1"</span>),</span><br><span class="line">   <span class="string">"b2"</span>: tf.Variable(tf.random_normal([n_hidden_2]), name=<span class="string">"b2"</span>),</span><br><span class="line">   <span class="string">"b3"</span>: tf.Variable(tf.random_normal([n_classes]), name=<span class="string">"b3"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 每一个需要在tensorboard上显示出来的单独模块都放在一个tf.name_scope()命名空间内</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"Model"</span>):</span><br><span class="line">   pred = multilayer_perceptron(X, weights, biases)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"Loss"</span>):</span><br><span class="line">   loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"SGD"</span>):</span><br><span class="line">   <span class="comment"># 正常自动进行反向传播,调用minimize方法时,底层进行的工作：</span></span><br><span class="line">   <span class="comment"># 计算trainable_variables集合中所有参数的梯度</span></span><br><span class="line">   <span class="comment"># 将梯度应用到变量上进行梯度下降</span></span><br><span class="line">   <span class="comment"># 在调用sess.run(train_op)时,会对variables进行更新</span></span><br><span class="line">   optimizer = tf.train.GradientDescentOptimizer(learning_rate)</span><br><span class="line">   <span class="comment"># 如果我们希望对梯度进行截断,那么就要自己计算出梯度,然后进行clip,最后对变量进行更新</span></span><br><span class="line">   <span class="comment"># tf.gradients计算出loss对tf.trainable_variables()中所有变量的梯度</span></span><br><span class="line">   grads = tf.gradients(loss, tf.trainable_variables())</span><br><span class="line">   <span class="comment"># 如果要截断梯度(防止梯度爆炸),要使用下面这句</span></span><br><span class="line">   <span class="comment"># 该函数先求所有梯度的L2范数,然后与15比较,如果大于则求缩放因子=15/L2范数</span></span><br><span class="line">   <span class="comment"># 最后将所有梯度乘以这个缩放因子，得到clipped_gradients</span></span><br><span class="line">   <span class="comment"># clipped_gradients, norm = tf.clip_by_global_norm(grads, 15)</span></span><br><span class="line">   <span class="comment"># 返回值clipped_gradients即裁剪后的梯度,norm为全局规约数</span></span><br><span class="line">   <span class="comment"># grads = list(zip(clipped_gradients, tf.trainable_variables()))</span></span><br><span class="line">   <span class="comment"># 将梯度和对应变量打包成元组,再转化成列表</span></span><br><span class="line">   grads = list(zip(grads, tf.trainable_variables()))</span><br><span class="line">   <span class="comment"># 用梯度更新变量</span></span><br><span class="line">   apply_grads = optimizer.apply_gradients(grads_and_vars=grads)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"Accuracy"</span>):</span><br><span class="line">   acc = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(Y, <span class="number">1</span>))</span><br><span class="line">   acc = tf.reduce_mean(tf.cast(acc, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.summary.scalar用来显示标量信息图</span></span><br><span class="line">tf.summary.scalar(<span class="string">"loss"</span>, loss)</span><br><span class="line">tf.summary.scalar(<span class="string">"accuracy"</span>, acc)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> tf.trainable_variables():</span><br><span class="line">   tf.summary.histogram(var.name, var)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> grad, var <span class="keyword">in</span> grads:</span><br><span class="line">   tf.summary.histogram(var.name + <span class="string">"/gradient"</span>, grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用来汇总所有summary信息,注意只有sess.run()之后才真正汇总</span></span><br><span class="line">merged_summary_op = tf.summary.merge_all()</span><br><span class="line"><span class="comment"># 日志书写器实例化，在实例化的同时将当前计算图写入日志</span></span><br><span class="line">summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   sess.run(tf.global_variables_initializer())</span><br><span class="line">   <span class="comment"># 训练模型</span></span><br><span class="line">   <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">      avg_cost = <span class="number">0.</span></span><br><span class="line">      total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">         batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">         <span class="comment"># merged_summary_op只有sess.run()之后才真正汇总所有summary信息</span></span><br><span class="line">         _, c, summary = sess.run([apply_grads, loss, merged_summary_op], feed_dict=&#123;X: batch_xs, Y: batch_ys&#125;)</span><br><span class="line">         <span class="comment"># add_summary仅仅是向FileWriter对象的缓存中存放每轮的event data,向disk上写数据是由FileWrite对象控制的</span></span><br><span class="line">         summary_writer.add_summary(summary, epoch * total_batch + i)</span><br><span class="line">         avg_cost += c / total_batch</span><br><span class="line">      <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">         print(<span class="string">"Epoch=&#123;:04d&#125;,cost=&#123;:.9f&#125;"</span>.format((epoch + <span class="number">1</span>), avg_cost))</span><br><span class="line">   <span class="comment"># 训练完成后,调用日志书写器实例对象summary_writer的close()方法将对象缓存(即所有记录的summary数据)写入文件</span></span><br><span class="line">   <span class="comment"># 否则它每隔120s自动写入一次</span></span><br><span class="line">   summary_writer.close()</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 测试模型</span></span><br><span class="line">   print(<span class="string">"Accuracy=&#123;&#125;"</span>.format(acc.eval(&#123;X: mnist.test.images, Y: mnist.test.labels&#125;)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要在tensorboard中查看数据,在保存日志文件的上一级目录中打开cmd.exe,输入下列命令:</span></span><br><span class="line"><span class="comment"># tensorboard --logdir=./tmp/</span></span><br><span class="line"><span class="comment"># 然后在浏览器中输入类似http://zgcr-win10-PC:6006这样的地址即可查看</span></span><br></pre></td></tr></table></figure>

<h1 id="Tensorflow-matplotlib可视化梯度下降"><a href="#Tensorflow-matplotlib可视化梯度下降" class="headerlink" title="Tensorflow+matplotlib可视化梯度下降"></a>Tensorflow+matplotlib可视化梯度下降</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"TF_CPP_MIN_LOG_LEVEL"</span>] = <span class="string">"2"</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"0"</span></span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">iteration = <span class="number">500</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># x和y数据</span></span><br><span class="line">real_params = [<span class="number">1.2</span>, <span class="number">2.5</span>]</span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">200</span>)[:, np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>, x_data.shape)</span><br><span class="line">y_data = x_data * real_params[<span class="number">0</span>] + real_params[<span class="number">1</span>] + noise</span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">Y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要训练的w和b初始化为5和4</span></span><br><span class="line">weight = tf.Variable(initial_value=[[<span class="number">5</span>]], dtype=tf.float32)</span><br><span class="line">bias = tf.Variable(initial_value=[[<span class="number">4</span>]], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">y = tf.matmul(X, weight) + bias</span><br><span class="line">loss = tf.losses.mean_squared_error(Y, y)</span><br><span class="line">train_op = tf.train.GradientDescentOptimizer(lr).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   sess.run(tf.global_variables_initializer())</span><br><span class="line">   w_record, b_record, loss_record = [], [], []</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(iteration):</span><br><span class="line">      w, b, cost, _ = sess.run([weight, bias, loss, train_op], feed_dict=&#123;X: x_data, Y: y_data&#125;)</span><br><span class="line">      w_record.append(w)</span><br><span class="line">      b_record.append(b)</span><br><span class="line">      loss_record.append(cost)</span><br><span class="line">   result = sess.run(y, feed_dict=&#123;X: x_data, Y: y_data&#125;)</span><br><span class="line"><span class="comment"># 画直线图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.scatter(x_data, y_data, s=<span class="number">1</span>, color=<span class="string">"r"</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.plot(x_data, result, lw=<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画3D图,图上三个坐标为w,b,loss值</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">ax_3d = Axes3D(fig)</span><br><span class="line"><span class="comment"># 构建三维曲面</span></span><br><span class="line">w_3d, b_3d = np.meshgrid(np.linspace(<span class="number">-4.2</span>, <span class="number">6.2</span>, <span class="number">50</span>), np.linspace(<span class="number">-2.5</span>, <span class="number">7.5</span>, <span class="number">50</span>))</span><br><span class="line">loss_3d = np.array(</span><br><span class="line">   [np.mean(np.square((x_data * w_ + b_) - y_data)) <span class="keyword">for</span> w_, b_ <span class="keyword">in</span> zip(w_3d.ravel(), b_3d.ravel())]).reshape(w_3d.shape)</span><br><span class="line"><span class="comment"># 画出三维曲面</span></span><br><span class="line">ax_3d.plot_surface(w_3d, b_3d, loss_3d, cmap=plt.get_cmap(<span class="string">"hot"</span>))</span><br><span class="line"><span class="comment"># .ravel()函数会直接修改原始矩阵,也是将矩阵降成一维</span></span><br><span class="line">w_record, b_record = np.array(w_record).ravel(), np.array(b_record).ravel()</span><br><span class="line"><span class="comment"># 根据w,b,loss的值,画出梯度下降曲线</span></span><br><span class="line">ax_3d.plot(w_record, b_record, loss_record, lw=<span class="number">10</span>, c=<span class="string">"blue"</span>)</span><br><span class="line"><span class="comment"># 梯度下降起始点</span></span><br><span class="line">ax_3d.scatter(w_record[<span class="number">0</span>], b_record[<span class="number">0</span>], loss_record[<span class="number">0</span>], s=<span class="number">20</span>, color=<span class="string">"black"</span>)</span><br><span class="line">ax_3d.set_xlabel(<span class="string">"w"</span>)</span><br><span class="line">ax_3d.set_ylabel(<span class="string">"b"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br></pre></td></tr></table></figure>

<h1 id="Tensorflow搭建线性回归神经网络"><a href="#Tensorflow搭建线性回归神经网络" class="headerlink" title="Tensorflow搭建线性回归神经网络"></a>Tensorflow搭建线性回归神经网络</h1><p>使用maplotlib画图时，如果有中文字符，一定要设置中文字体，因为matplotlib默认字体不支持中文，无法正常显示。<br>最好设置输入X和Y的占位符变量，这样我们可以使用feed_dict来灵活地定制要输入的样本数据。如果我们的样本数据是list形式，在输入到feed_dict后能够自动转换成相应的numpy数组，可直接进行tensorflow中的计算。<br>创建会话后一定要先初始化所有变量后才可以开始迭代训练模型！<br>如果我们想获得训练过程中的某项数据，找到计算图中定义的该数据变量名，sess.run()它即可，即可获得计算得到的数据(是numpy数组形式)，注意run时看看上面是否载入了feed_dict的样本数据，没载入的话还得重新载入一次。<br>使用matplotlib画图时必须使用上面sess.run()后得到的计算结果(是numpy数组形式)，tensorflow的张量是不能用来绘图的(张量相当于是个引用，直接print张量只能得到张量的属性信息)!<br>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 用于画图时显示中文字符</span></span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> mpl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义matplotlib画图时的全局使用仿宋中文字体，如果不定义成中文字体，有中文时不能正常显示</span></span><br><span class="line">mpl.rcParams[<span class="string">"font.sans-serif"</span>] = [<span class="string">"FangSong"</span>]</span><br><span class="line"><span class="comment"># 下面的设置可以使这个警告不出现:Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2</span></span><br><span class="line">os.environ[<span class="string">"TF_CPP_MIN_LOG_LEVEL"</span>] = <span class="string">"2"</span></span><br><span class="line"><span class="comment"># 指定使用的GPU的编号，从0开始</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"0"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">learning_rate = <span class="number">0.0005</span></span><br><span class="line">iteration = <span class="number">5000</span></span><br><span class="line">display_step = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集数据</span></span><br><span class="line">sample_number = <span class="number">1000</span></span><br><span class="line"><span class="comment"># x值从以0为均值,以0.55为标准差的高斯分布中取随机数</span></span><br><span class="line">train_x = [np.random.normal(<span class="number">0.0</span>, <span class="number">0.55</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(sample_number)]</span><br><span class="line"><span class="comment"># y值在y1=x1*0.1+0.3上小范围浮动</span></span><br><span class="line">train_y = [train_x[i] * <span class="number">0.1</span> + <span class="number">0.3</span> + np.random.normal(<span class="number">0.0</span>, <span class="number">0.03</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(sample_number)]</span><br><span class="line">plt.scatter(train_x, train_y, s=<span class="number">5</span>, color=<span class="string">"blue"</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">train_x = np.array(train_x)</span><br><span class="line">train_y = np.array(train_y)</span><br><span class="line">print(train_x.shape, train_y.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义X和Y占位符</span></span><br><span class="line">X = tf.placeholder(tf.float32)</span><br><span class="line">Y = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型权重参数,随机分布初始化和零值初始化</span></span><br><span class="line"><span class="comment"># W权重为[1,10]个元素的矩阵,矩阵元素Weight全部从标准正态分布中随机去除的数</span></span><br><span class="line"><span class="comment"># 生成1维的W矩阵,取值是均匀分布[-1,1]之间的随机数;# 生成1维的b矩阵，初始值是0</span></span><br><span class="line">W = tf.Variable(tf.random_uniform([<span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>), name=<span class="string">'W'</span>)</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>]), name=<span class="string">'b'</span>)</span><br><span class="line"><span class="comment"># 定义线性回归模型</span></span><br><span class="line">pred = tf.add(W * X, b)</span><br><span class="line"><span class="comment"># 定义损失函数为均方误差损失函数</span></span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(pred - Y), name=<span class="string">"loss"</span>))</span><br><span class="line"><span class="comment"># 定义优化算法为梯度下降</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, name=<span class="string">"train"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   <span class="comment"># 初始化计算图所有变量</span></span><br><span class="line">   sess.run(tf.global_variables_initializer())</span><br><span class="line">   <span class="comment"># 打印初始化的变量</span></span><br><span class="line">   print(<span class="string">"W=&#123;&#125;,b=&#123;&#125;"</span>.format(sess.run(W), sess.run(b)))</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(iteration):</span><br><span class="line">      sess.run(optimizer, feed_dict=&#123;X: train_x, Y: train_y&#125;)</span><br><span class="line">      <span class="comment"># 每迭代50轮,计算loss值</span></span><br><span class="line">      <span class="keyword">if</span> (i + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">         cost = sess.run(loss, feed_dict=&#123;X: train_x, Y: train_y&#125;)</span><br><span class="line">         print(<span class="string">"Epoch=&#123;:04d&#125;,cost=&#123;:.9f&#125;,w=&#123;&#125;,b=&#123;&#125;"</span>.format((i + <span class="number">1</span>), cost, sess.run(W), sess.run(b)))</span><br><span class="line">   <span class="comment"># 注意这里已经退出了上面的循环，因此再次sess.run时要重新设置feed_dict</span></span><br><span class="line">   pred_, cost = sess.run([pred, loss], feed_dict=&#123;X: train_x, Y: train_y&#125;)</span><br><span class="line">   W_, b_ = sess.run(W), sess.run(b)</span><br><span class="line">   print(<span class="string">"Training complete,cost=&#123;&#125;,W=&#123;&#125;,b=&#123;&#125;"</span>.format(cost, W_, b_))</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 显示数据点和预测曲线</span></span><br><span class="line">   plt.scatter(train_x, train_y, s=<span class="number">5</span>, color=<span class="string">"blue"</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">   plt.plot(train_x, pred_, label=<span class="string">"模型预测曲线"</span>)</span><br><span class="line">   <span class="comment"># 显示图例</span></span><br><span class="line">   plt.legend()</span><br><span class="line">   plt.show()</span><br></pre></td></tr></table></figure>

<h1 id="Tensorflow搭建第一个逻辑回归神经网络-mnist-手写体数字分类"><a href="#Tensorflow搭建第一个逻辑回归神经网络-mnist-手写体数字分类" class="headerlink" title="Tensorflow搭建第一个逻辑回归神经网络(mnist)手写体数字分类"></a>Tensorflow搭建第一个逻辑回归神经网络(mnist)手写体数字分类</h1><p>神经网络搭建过程：<br>下载mnist数据集；<br>设置超参数；<br>设置x和y占位符用来接收输入的样本数据和样本标签；<br>定义网络权重和网络计算过程；<br>定义loss函数和优化算法；<br>创建会话，开始训练；<br>训练完毕后，测试模型，计算准确率。<br><strong>代码实现如下：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面的设置可以使这个警告不出现:Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2</span></span><br><span class="line">os.environ[<span class="string">"TF_CPP_MIN_LOG_LEVEL"</span>] = <span class="string">"2"</span></span><br><span class="line"><span class="comment"># 指定使用的GPU的编号，从0开始</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"0"</span></span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"/data/"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.05</span></span><br><span class="line">training_epochs = <span class="number">25</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># x和y占位符,mnist数据集图片shape为28*28=784,标签为one_hot编码</span></span><br><span class="line">X = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">Y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络权重</span></span><br><span class="line"><span class="comment"># tf.random_normal从正态分布中取随机数,tf.truncated_normal从截断正态分布中取随机数,tf.constant取指定的常数</span></span><br><span class="line">W = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"><span class="comment"># 定义网络计算过程</span></span><br><span class="line">pred = tf.nn.softmax(tf.matmul(X, W) + b)</span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(pred), axis=<span class="number">0</span>))</span><br><span class="line"><span class="comment"># 优化算法为梯度下降</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   <span class="comment"># 开始计算前初始化所有变量</span></span><br><span class="line">   sess.run(tf.global_variables_initializer())</span><br><span class="line">   <span class="comment"># 训练模型</span></span><br><span class="line">   <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">      avg_cost = <span class="number">0.</span></span><br><span class="line">      total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">         batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">         _, c = sess.run([optimizer, cost], feed_dict=&#123;X: batch_xs, Y: batch_ys&#125;)</span><br><span class="line">         avg_cost += c / total_batch</span><br><span class="line">      <span class="comment"># 每轮epoch打印cost值</span></span><br><span class="line">      <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">         print(<span class="string">"Epoch=&#123;:04d&#125;,cost=&#123;:.9f&#125;"</span>.format((epoch + <span class="number">1</span>), avg_cost))</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 计算模型准确率</span></span><br><span class="line">   <span class="comment"># tf.argmax函数对在指定维度上取最大值的下标,这里即在第1个维度上取最大值的下标</span></span><br><span class="line">   <span class="comment"># 也就是模型预测的是哪个数字的概率最大,其下标就是对应的这个数字</span></span><br><span class="line">   <span class="comment"># tf.equal(A,B)是对比两个矩阵同位置上元素,相等返回True,否则False,返回的也是和A一样维度的矩阵</span></span><br><span class="line">   correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(Y, <span class="number">1</span>))</span><br><span class="line">   <span class="comment"># tf.cast将bool型的True和False转换为1和0,这样就可以计算准确率了</span></span><br><span class="line">   accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">   <span class="comment"># .eval()相当于Sess.run()一个Tensor对象,得到一个numpy数组结果</span></span><br><span class="line">   print(<span class="string">"Accuracy=&#123;&#125;"</span>.format(accuracy.eval(&#123;X: mnist.test.images, Y: mnist.test.labels&#125;)))</span><br></pre></td></tr></table></figure>

<h1 id="Tensorflow搭建第一个CNN神经网络-mnist-手写体数字分类"><a href="#Tensorflow搭建第一个CNN神经网络-mnist-手写体数字分类" class="headerlink" title="Tensorflow搭建第一个CNN神经网络(mnist)手写体数字分类"></a>Tensorflow搭建第一个CNN神经网络(mnist)手写体数字分类</h1><p>这里我们使用上面介绍的tf.data方式加载数据，可在每轮训练时就预加载下一轮的训练数据，训练起来速度更快。<br>如果我们想把训练集和测试集分开，那么需要对训练集和测试集分别创建dataset对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">display_step = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mnist样本为一维的28*28=784向量,标签为one_hot编码</span></span><br><span class="line">n_input = <span class="number">784</span></span><br><span class="line">class_number = <span class="number">10</span></span><br><span class="line">drop = <span class="number">0.75</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用tf.data方式加载训练集和测试集</span></span><br><span class="line">train_dataset = tf.data.Dataset.from_tensor_slices((mnist.train.images, mnist.train.labels)).repeat().batch(</span><br><span class="line">   batch_size)</span><br><span class="line">train_dataset = train_dataset.prefetch(buffer_size=batch_size)</span><br><span class="line">train_iterator = train_dataset.make_initializable_iterator()</span><br><span class="line">train_batch_x, train_batch_y = train_iterator.get_next()</span><br><span class="line"></span><br><span class="line">test_dataset = tf.data.Dataset.from_tensor_slices((mnist.test.images, mnist.test.labels)).repeat().batch(</span><br><span class="line">   batch_size)</span><br><span class="line">test_iterator = test_dataset.make_initializable_iterator()</span><br><span class="line">test_batch_x, test_batch_y = test_iterator.get_next()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_net</span><span class="params">(x, n_classes, dropout, reuse, is_training)</span>:</span></span><br><span class="line">   <span class="comment"># is_training参数表示是在训练/测试,reuse参数为True获取变量,False为创建变量</span></span><br><span class="line">   <span class="keyword">with</span> tf.variable_scope(<span class="string">"ConvNet"</span>, reuse=reuse):</span><br><span class="line">      <span class="comment"># mnist图片数据为1维784向量,需要reshape成[-1, 28, 28, 1]</span></span><br><span class="line">      <span class="comment"># 即tensorflow需要的维度格式:[Batch Size, Height, Width, Channel]</span></span><br><span class="line">      x = tf.reshape(x, shape=[<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line">      <span class="comment"># 32层卷积,卷积核5x5,卷积核参数自动初始化</span></span><br><span class="line">      conv1 = tf.layers.conv2d(x, <span class="number">32</span>, <span class="number">5</span>, activation=tf.nn.relu)</span><br><span class="line">      <span class="comment"># 最大池化,2x2,步长2</span></span><br><span class="line">      pool1 = tf.layers.max_pooling2d(conv1, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">      <span class="comment"># 64层卷积,卷积核3x3</span></span><br><span class="line">      conv2 = tf.layers.conv2d(pool1, <span class="number">64</span>, <span class="number">3</span>, activation=tf.nn.relu)</span><br><span class="line">      <span class="comment"># 最大池化,2x2,步长2</span></span><br><span class="line">      pool2 = tf.layers.max_pooling2d(conv2, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">      <span class="comment"># 将得到的特征图拉平为1维数据</span></span><br><span class="line">      fc1 = tf.contrib.layers.flatten(pool2)</span><br><span class="line">      <span class="comment"># 全连接层,只需要定义全连接层权重数,自动匹配上一级拉平的1维特征图尺寸做矩阵乘法</span></span><br><span class="line">      fc1 = tf.layers.dense(fc1, <span class="number">1024</span>)</span><br><span class="line">      <span class="comment"># 训练时激活dropout层</span></span><br><span class="line">      fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)</span><br><span class="line">      <span class="comment"># 输出层</span></span><br><span class="line">      out = tf.layers.dense(fc1, n_classes)</span><br><span class="line">      <span class="comment"># 只在测试时计算softmax函数输出值,因为损失函数附带计算过了</span></span><br><span class="line">      out = tf.nn.softmax(out) <span class="keyword">if</span> <span class="keyword">not</span> is_training <span class="keyword">else</span> out</span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立训练和测试网络</span></span><br><span class="line"><span class="comment"># reuse=False时,作用域就是为创建新变量所设置的;reuse=True时,作用域是为重用变量所设置</span></span><br><span class="line">logits_train = conv_net(train_batch_x, class_number, drop, reuse=<span class="literal">False</span>, is_training=<span class="literal">True</span>)</span><br><span class="line">logits_test = conv_net(test_batch_x, class_number, drop, reuse=<span class="literal">True</span>, is_training=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 定义loss函数为交叉熵函数</span></span><br><span class="line">loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_train, labels=train_batch_y))</span><br><span class="line"><span class="comment"># 定义优化算法为Adam算法</span></span><br><span class="line">train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_op)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">correct_pred = tf.equal(tf.argmax(logits_test, <span class="number">1</span>), tf.argmax(test_batch_y, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   sess.run(tf.global_variables_initializer())</span><br><span class="line">   sess.run(train_iterator.initializer)</span><br><span class="line">   sess.run(test_iterator.initializer)</span><br><span class="line">   <span class="comment"># 训练模型</span></span><br><span class="line">   <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">      sess.run(train_op)</span><br><span class="line">      <span class="comment"># 每训练100个batch测试一下模型</span></span><br><span class="line">      <span class="keyword">if</span> step % display_step == <span class="number">0</span> <span class="keyword">or</span> step == <span class="number">0</span>:</span><br><span class="line">         loss, acc = sess.run([loss_op, accuracy])</span><br><span class="line">         print(<span class="string">"step=&#123;&#125;,loss=&#123;&#125;,test accuracy=&#123;&#125;"</span>.format(step, loss, acc))</span><br></pre></td></tr></table></figure>

<h1 id="Tensorflow使用高级API-tf-Estimators搭建CNN神经网络-mnist手写体数字分类"><a href="#Tensorflow使用高级API-tf-Estimators搭建CNN神经网络-mnist手写体数字分类" class="headerlink" title="Tensorflow使用高级API tf.Estimators搭建CNN神经网络(mnist手写体数字分类)"></a>Tensorflow使用高级API tf.Estimators搭建CNN神经网络(mnist手写体数字分类)</h1><p>下面的代码使用了tensorflow高级API tf.Estimators，训练时速度更快。具体代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 传统方式下载mnist数据集</span></span><br><span class="line"><span class="comment"># from tensorflow.examples.tutorials.mnist import input_data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面的设置可以使这个警告不出现:Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2</span></span><br><span class="line">os.environ[<span class="string">"TF_CPP_MIN_LOG_LEVEL"</span>] = <span class="string">"2"</span></span><br><span class="line"><span class="comment"># 指定使用的GPU的编号，从0开始</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"0"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 传统方式加载mnist数据集</span></span><br><span class="line"><span class="comment"># mnist = input_data.read_data_sets("MNIST_data", one_hot=False)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载mnist数据集</span></span><br><span class="line">mnist = tf.contrib.learn.datasets.load_dataset(<span class="string">"mnist"</span>)</span><br><span class="line">train_data = mnist.train.images</span><br><span class="line">train_labels = np.asarray(mnist.train.labels, dtype=np.int32)</span><br><span class="line">eval_data = mnist.test.images</span><br><span class="line">eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">num_steps = <span class="number">2000</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">num_input = <span class="number">784</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line">drop = <span class="number">0.25</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络模型,这里采用高级API Estimators的形式定义模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_net</span><span class="params">(x_dict, n_classes, dropout, reuse, is_training)</span>:</span></span><br><span class="line">   <span class="comment"># is_training参数表示是在训练/测试,reuse参数为True获取变量,False为创建变量</span></span><br><span class="line">   <span class="keyword">with</span> tf.variable_scope(<span class="string">"ConvNet"</span>, reuse=reuse):</span><br><span class="line">      x = x_dict[<span class="string">"images"</span>]</span><br><span class="line">      <span class="comment"># mnist图片数据为1维784向量,需要reshape成[-1, 28, 28, 1]</span></span><br><span class="line">      <span class="comment"># 即tensorflow需要的维度格式:[Batch Size, Height, Width, Channel]</span></span><br><span class="line">      x = tf.reshape(x, shape=[<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line">      <span class="comment"># 32层卷积,卷积核5x5,卷积核参数自动初始化</span></span><br><span class="line">      conv_1 = tf.layers.conv2d(x, <span class="number">32</span>, <span class="number">5</span>, activation=tf.nn.relu)</span><br><span class="line">      <span class="comment"># 最大池化,2x2,步长2</span></span><br><span class="line">      pool_1 = tf.layers.max_pooling2d(conv_1, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">      <span class="comment"># 64层卷积,卷积核3x3</span></span><br><span class="line">      conv_2 = tf.layers.conv2d(pool_1, <span class="number">64</span>, <span class="number">3</span>, activation=tf.nn.relu)</span><br><span class="line">      <span class="comment"># 最大池化,2x2,步长2</span></span><br><span class="line">      pool_2 = tf.layers.max_pooling2d(conv_2, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">      <span class="comment"># 将得到的特征图拉平为1维数据</span></span><br><span class="line">      fc1 = tf.contrib.layers.flatten(pool_2)</span><br><span class="line">      <span class="comment"># 全连接层,只需要定义全连接层权重数,自动匹配上一级拉平的1维特征图尺寸做矩阵乘法</span></span><br><span class="line">      fc1 = tf.layers.dense(fc1, <span class="number">1024</span>)</span><br><span class="line">      <span class="comment"># 训练时激活dropout层</span></span><br><span class="line">      fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)</span><br><span class="line">      <span class="comment"># 输出层</span></span><br><span class="line">      out = tf.layers.dense(fc1, n_classes)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义的model_fn</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode)</span>:</span></span><br><span class="line">   <span class="comment"># 根据训练和测试的不同需要建立两个网络,训练时要新建变量,同时dropout层要激活</span></span><br><span class="line">   logits_train = conv_net(features, num_classes, drop, reuse=<span class="literal">False</span>, is_training=<span class="literal">True</span>)</span><br><span class="line">   logits_test = conv_net(features, num_classes, drop, reuse=<span class="literal">True</span>, is_training=<span class="literal">False</span>)</span><br><span class="line">   <span class="comment"># 定义损失函数为交叉熵,优化算法为Adam,原始labels转为0、1二值</span></span><br><span class="line">   loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">      logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))</span><br><span class="line">   <span class="comment"># tf.train.get_global_step()获取global_step,这个变量是Tensorflow随着训练过程自动更新的</span></span><br><span class="line">   train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_op,</span><br><span class="line">                                                                           global_step=tf.train.get_global_step())</span><br><span class="line">   <span class="comment"># 预测结果</span></span><br><span class="line">   pred_classes = tf.argmax(logits_test, axis=<span class="number">1</span>)</span><br><span class="line">   <span class="comment"># 计算准确率</span></span><br><span class="line">   acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)</span><br><span class="line">   <span class="keyword">if</span> mode == tf.estimator.ModeKeys.PREDICT:</span><br><span class="line">      <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, predictions=pred_classes)</span><br><span class="line">   estim_specs = tf.estimator.EstimatorSpec(mode=mode, predictions=pred_classes, loss=loss_op, train_op=train_op,</span><br><span class="line">                                            eval_metric_ops=&#123;<span class="string">'accuracy'</span>: acc_op&#125;)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> estim_specs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立estimator</span></span><br><span class="line">model = tf.estimator.Estimator(model_fn)</span><br><span class="line"><span class="comment"># 定义训练用样本的数据</span></span><br><span class="line">input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;<span class="string">"images"</span>: mnist.train.images&#125;, y=mnist.train.labels,</span><br><span class="line">                                              batch_size=batch_size, num_epochs=num_epochs, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model.train(input_fn, steps=num_steps)</span><br><span class="line"><span class="comment"># 定义测试用样本的数据</span></span><br><span class="line">input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;<span class="string">"images"</span>: mnist.test.images&#125;, y=mnist.test.labels,</span><br><span class="line">                                              batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line">el = model.evaluate(input_fn)</span><br><span class="line">print(<span class="string">"Testing Accuracy=&#123;&#125;"</span>.format(el[<span class="string">"accuracy"</span>]))</span><br></pre></td></tr></table></figure>

<h1 id="Tensorflow搭建第一个LSTM神经网络-mnist手写体数字分类"><a href="#Tensorflow搭建第一个LSTM神经网络-mnist手写体数字分类" class="headerlink" title="Tensorflow搭建第一个LSTM神经网络(mnist手写体数字分类)"></a>Tensorflow搭建第一个LSTM神经网络(mnist手写体数字分类)</h1><p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib <span class="keyword">import</span> rnn</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"TF_CPP_MIN_LOG_LEVEL"</span>] = <span class="string">"2"</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"0"</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">training_steps = <span class="number">10000</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">test_size = <span class="number">128</span></span><br><span class="line">display_step = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个时刻输入一行上28个像素的数据</span></span><br><span class="line">num_input = <span class="number">28</span></span><br><span class="line"><span class="comment"># 分为28个时刻</span></span><br><span class="line">time_steps = <span class="number">28</span></span><br><span class="line"><span class="comment"># 神经元数量</span></span><br><span class="line">num_hidden = <span class="number">128</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">X, Y = tf.placeholder(<span class="string">"float"</span>, [<span class="literal">None</span>, time_steps, num_input]), tf.placeholder(<span class="string">"float"</span>, [<span class="literal">None</span>, num_classes])</span><br><span class="line"></span><br><span class="line">weights = &#123;<span class="string">"out"</span>: tf.Variable(tf.random_normal([num_hidden, num_classes]))&#125;</span><br><span class="line">biases = &#123;<span class="string">"out"</span>: tf.Variable(tf.random_normal([num_classes]))&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_net</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">   <span class="comment"># tf.stack()是一个矩阵拼接函数,tf.unstack()是一个矩阵分解函数</span></span><br><span class="line">   <span class="comment"># 输入数据的shape:[batch_size, time_steps, n_input]</span></span><br><span class="line">   <span class="comment"># tf.unstack()将给定的R维张量拆分成R-1维张量,即将x按axis=1分解成time_steps个张量,最后返回time_steps个张量组成的列表</span></span><br><span class="line">   x = tf.unstack(x, time_steps, axis=<span class="number">1</span>)</span><br><span class="line">   <span class="comment"># 定义LSTM层,该层有128个神经元,forget_bias=1.0即初始第一个时刻遗忘门的偏置,这是为了减少在开始训练时遗忘的规模</span></span><br><span class="line">   <span class="comment"># state_is_tuple默认为True,就是表示返回的每个时刻输出ht和每个时刻的记忆Ct用一个元组表示</span></span><br><span class="line">   lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=<span class="number">1.0</span>, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   <span class="comment"># rnn.static_rnn()的输出对应于每一个时刻,如果只关心最后一个时刻的输出,取outputs[-1]即可</span></span><br><span class="line">   <span class="comment"># outputs即每个时刻的输出ht,states即每个时刻的记忆Ct</span></span><br><span class="line">   outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)</span><br><span class="line">   <span class="comment"># 去最后一个时刻的ht输出,进行一次线性变换</span></span><br><span class="line">   <span class="keyword">return</span> tf.matmul(outputs[<span class="number">-1</span>], w[<span class="string">"out"</span>]) + b[<span class="string">"out"</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建rnn网络</span></span><br><span class="line">logits = rnn_net(X, weights, biases)</span><br><span class="line"><span class="comment"># 最终预测结果</span></span><br><span class="line">prediction = tf.nn.softmax(logits)</span><br><span class="line"><span class="comment"># 损失函数为交叉熵,优化算法使用Adam算法</span></span><br><span class="line">loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))</span><br><span class="line">train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_op)</span><br><span class="line"><span class="comment"># 计算预测准确率</span></span><br><span class="line">correct_pred = tf.equal(tf.argmax(prediction, <span class="number">1</span>), tf.argmax(Y, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   sess.run(tf.global_variables_initializer())</span><br><span class="line">   <span class="comment"># 训练模型</span></span><br><span class="line">   <span class="keyword">for</span> step <span class="keyword">in</span> range(training_steps):</span><br><span class="line">      train_batch_x, train_batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">      <span class="comment"># 将输入数据reshape成[batch_size, time_steps, n_input]</span></span><br><span class="line">      train_batch_x = train_batch_x.reshape((batch_size, time_steps, num_input))</span><br><span class="line">      sess.run(train_op, feed_dict=&#123;X: train_batch_x, Y: train_batch_y&#125;)</span><br><span class="line">      <span class="comment"># 每迭代10轮打印loss值和预测准确率</span></span><br><span class="line">      <span class="keyword">if</span> step % display_step == <span class="number">0</span> <span class="keyword">or</span> step == <span class="number">0</span>:</span><br><span class="line">         test_batch_x, test_batch_y = mnist.test.next_batch(batch_size)</span><br><span class="line">         test_batch_x = test_batch_x.reshape((batch_size, time_steps, num_input))</span><br><span class="line">         loss, acc = sess.run([loss_op, accuracy], feed_dict=&#123;X: test_batch_x, Y: test_batch_y&#125;)</span><br><span class="line">         print(<span class="string">"step=&#123;&#125;,loss=&#123;:.4f&#125;,training accuracy=&#123;:.3f&#125;"</span>.format(step, loss, acc))</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 测试模型</span></span><br><span class="line">   test_batch_x, test_batch_y = mnist.test.next_batch(batch_size)</span><br><span class="line">   test_batch_x = test_batch_x.reshape((batch_size, time_steps, num_input))</span><br><span class="line">   print(<span class="string">"Testing Accuracy=&#123;&#125;"</span>.format(sess.run(accuracy, feed_dict=&#123;X: test_batch_x, Y: test_batch_y&#125;)))</span><br></pre></td></tr></table></figure>
            </div>
            <hr/>

            
            <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.88rem;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff;
        background-color: #22AB38;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff;
        background-color: #019FE8;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a class="reward-link btn-floating btn-large waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close"><i class="fa fa-close"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs">
                        <li class="tab wechat-tab waves-effect waves-light"><a class="active" href="#wechat">微信</a></li>
                        <li class="tab alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                    </ul>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('#reward .reward-link').on('click', function () {
            $('#rewardModal').openModal();
        });

        $('#rewardModal .close').on('click', function () {
            $('#rewardModal').closeModal();
        });
    });
</script>
            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone" data-wechat-qrcode-helper="<p>微信里点“发现”->“扫一扫”二维码便可查看分享。</p>"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            <div class="reprint">
                <p>
                    <span class="reprint-tip">
                        <i class="fa fa-exclamation-circle"></i>&nbsp;&nbsp;Reprint please specify:
                    </span>
                    <a href="https://wyg1996.cn" class="b-link-green">修改</a>
                    <i class="fa fa-angle-right fa-lg fa-fw text-color"></i>
                    <a href="/2019/03/03/Tensorflow基础知识介绍、tensorflow搭建基础神经网络合集/" class="b-link-green">Tensorflow基础知识介绍、Tensorflow搭建基础神经网络合集</a>
                </p>
            </div>
        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '',
        clientSecret: '',
        repo: '',
        owner: '',
        admin: null,
        id: '2019-03-03T18-37-38',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    
        <link rel="stylesheet" href="/libs/gitment/gitment-default.css">
<link rel="stylesheet" href="/css/gitment.css">

<div class="gitment-card card" data-aos="fade-up">
    <div id="gitment-content" class="card-content"></div>
</div>

<script src="/libs/gitment/gitment.js"></script>
<script>
var gitment = new Gitment({
    id: 'Sun Mar 03 2019 18:37:38 GMT+0800',
    owner: '',
    repo: '',
    oauth: {
        client_id: '',
        client_secret: ''
    }
});

gitment.render('gitment-content');
</script>
    

    
        <div class="disqus-card card" data-aos="fade-up">
    <div id="disqus_thread" class="card-content">
        <noscript>Please enable JavaScript to view the
            <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
    </div>
</div>

<script type="text/javascript">
    disqus_config = function () {
        this.page.url = 'https://wyg1996.cn/2019/03/03/Tensorflow基础知识介绍、tensorflow搭建基础神经网络合集/';
        this.page.identifier = '/2019/03/03/Tensorflow基础知识介绍、tensorflow搭建基础神经网络合集/';
        this.page.title = 'Tensorflow基础知识介绍、Tensorflow搭建基础神经网络合集';
    };
    let disqus_shortname = '';

    (function () { // DON'T EDIT BELOW THIS LINE
        let d = document, s = d.createElement('script');
        // 如：s.src = 'https://blinkfox.disqus.com/embed.js';
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
    

    
    <div class="livere-card card" data-aos="fade-up">
    <!-- 来必力City版安装代码 -->
    <div id="lv-container" class="card-content" data-id="city" data-uid="">
        <script type="text/javascript">
            (function (d, s) {
                let j, e = d.getElementsByTagName(s)[0];
                if (typeof LivereTower === 'function') {
                    return;
                }

                j = d.createElement(s);
                j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
                j.async = true;

                e.parentNode.insertBefore(j, e);
            })(document, 'script');
        </script>
        <noscript>为正常使用来必力评论功能请激活JavaScript。</noscript>
    </div>
    <!-- City版安装代码已完成 -->
</div>
    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2019/03/04/使用深度学习的CNN神经网络破解Captcha验证码/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/20.jpg" class="responsive-img" alt="使用深度学习的CNN神经网络破解Captcha验证码">
                        
                        <span class="card-title">使用深度学习的CNN神经网络破解Captcha验证码</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">Captcha样本数据的生成与处理我们熟知的mnist数据集中的每张图片是一个(784,)的一维向量。同时，向量中的每个值都/255进行了0-1二值化。其标签是one_hot编码，即一个(10,1)的一维向量，假如该图片是7，那么这个向量中</div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="fa fa-clock-o fa-fw icon-date"></i>2019-03-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/深度学习算法实践/" class="post-category" target="_blank">
                                    深度学习算法实践
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/深度学习算法实践/" target="_blank">
                        <span class="chip bg-color">深度学习算法实践</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2019/03/02/pytorch基础知识介绍、pytorch搭建基础神经网络合集/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/11.jpg" class="responsive-img" alt="pytorch基础知识介绍、pytorch搭建基础神经网络合集">
                        
                        <span class="card-title">pytorch基础知识介绍、pytorch搭建基础神经网络合集</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">pytorch介绍PyTorch可以追溯到2002年诞生的Torch。Torch是一个与Numpy类似的张量(Tensor)操作库，它使用了一种不是很大众的语言Lua作为接口。在2017年，Torch的幕后团队推出了PyTorch。PyTo</div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2019-03-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Pytorch/" class="post-category" target="_blank">
                                    Pytorch
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Pytorch/" target="_blank">
                        <span class="chip bg-color">Pytorch</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + 'From: 修改<br />'
            + 'Author: ygwu<br />'
            + 'Link: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>

    </div>
    <div class="col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });
    });
</script>
    

</main>


<footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            &copy;<a href="mailto:ygwu@mail.ustc.edu.cn" target="_blank">鱼缸屋</a>
            

            
                &nbsp;<i class="fa fa-area-chart"></i>&nbsp;站点总字数:&nbsp;
                <span class="white-color">393.7k</span>
            

            
			
                <br>
                
                <span id="busuanzi_container_site_pv">
                    <i class="fa fa-heart-o"></i>
                    本站总访问量 <span id="busuanzi_value_site_pv" class="white-color"></span>
                </span>
                
                
                <span id="busuanzi_container_site_uv">
                    <i class="fa fa-users"></i>
                    次,&nbsp;访客数 <span id="busuanzi_value_site_uv" class="white-color"></span> 人.
                </span>
                
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/wygny" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:ygwu@mail.ustc.edu.cn" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=321699849" class="tooltipped" data-tooltip="QQ联系我: 321699849" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>



    <a href="https://www.baidu.com" class="tooltipped" target="_blank" data-tooltip="访问百度" data-position="top" data-delay="50">
        <i class="fa fa-paw"></i>
</a>



    <a href="https://www.google.com" class="tooltipped" target="_blank" data-tooltip="访问谷歌" data-position="top" data-delay="50">
        <i class="fa fa-google"></i>
</a>



    <a href="https://www.bilibili.com" class="tooltipped" target="_blank" data-tooltip="访问哔哩哔哩" data-position="top" data-delay="50">
        <i class="fa fa-bullseye"></i>
</a>



    <a href="https://blog.csdn.net/zgcr654321" class="tooltipped" target="_blank" data-tooltip="访问我的CSDN博客" data-position="top" data-delay="50">
        <i class="fa fa-copyright"></i>
</a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>




</div>
    </div>
</footer>

<div class="progress-bar"></div>


<!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input" autofocus="">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
</script>
<!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


<script src="/libs/materialize/js/materialize.min.js"></script>
<script src="/libs/masonry/masonry.pkgd.min.js"></script>
<script src="/libs/aos/aos.js"></script>
<script src="/libs/scrollprogress/scrollProgress.min.js"></script>
<script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
<script src="/js/matery.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->



    <script src="/libs/others/clicklove.js"></script>


    <script async src="/libs/others/busuanzi.pure.mini.js"></script>


</body>
</html>